{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqTF45Y5UVwh"
      },
      "source": [
        "Google drive connection, installs necessary for notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0rMEkF9UQzL",
        "outputId": "8ab01d4c-d620-48df-ed36-0eae1798128f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla P100-PCIE-16GB (UUID: GPU-9e706f4e-bb82-e429-aba5-3cad6aa6833b)\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi -L"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOGwVr97aTBK",
        "outputId": "0207e834-ce7c-4e1e-98b0-22beae2f99ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/gdrive')\n",
        "    os.chdir(\"/content/gdrive/MyDrive/NBME/input\")\n",
        "\n",
        "!pip install -qq --upgrade pip\n",
        "!pip install transformers sentencepiece datasets -qq --no-cache-dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjSXLlxUUIpl"
      },
      "outputs": [],
      "source": [
        "class ModelParams:\n",
        "    pt_batch_size = 16      # Batch size when pre-training with MLM\n",
        "    init_batch_size = 16    # For when model has just our beginning supervised data\n",
        "    fin_batch_size = 32     # For when model has semi-supervised data\n",
        "    eval_batch_size = 40    # For model evaluation, (no backward pass)\n",
        "    sub_batch_size = 4      # For large transformers GPU RAM can only handle batch sizes of 4 during MLM\n",
        "    max_length = 460        # Max length found from previous tokenizations\n",
        "    rand_seed = 42\n",
        "    num_workers = 2         # setting num_workers > 0 can sometimes cause freezes\n",
        "    pin_mem = True if \\\n",
        "      num_workers > 0 else False\n",
        "    num_epochs = 12\n",
        "    n_folds = 5             # Num folds for cross validation\n",
        "    model_type = 'roberta'  # Fine tune the different models on separate notebooks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjeJLN0bUIpS"
      },
      "source": [
        "# Imports, Utils, and Files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Juv2fVo_UIpc"
      },
      "source": [
        "Fast tokenizer borrowed from Thanh Nguyen's Fast Tokenizer with files [here](https://www.kaggle.com/datasets/thanhns/deberta-v2-3-fast-tokenizer) and [here](https://www.kaggle.com/datasets/thanhns/deberta-tokenizer)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9tfn8mIZUIpe"
      },
      "outputs": [],
      "source": [
        "# The following is necessary if you want to use the fast tokenizer for deberta v2 or v3\n",
        "# This must be done before importing transformers\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "import transformers\n",
        "\n",
        "transformers_path = Path(transformers.__file__.replace(\"/__init__.py\", \"\"))\n",
        "input_dir = Path(\"../input/deberta-v2-3-fast-tokenizer\")\n",
        "\n",
        "convert_file = input_dir / \"convert_slow_tokenizer.py\"\n",
        "conversion_path = transformers_path/convert_file.name\n",
        "\n",
        "if conversion_path.exists(): conversion_path.unlink()\n",
        "\n",
        "shutil.copy(convert_file, transformers_path)\n",
        "deberta_v2_path = transformers_path / \"models\" / \"deberta_v2\"\n",
        "\n",
        "for filename in ['tokenization_deberta_v2.py', 'tokenization_deberta_v2_fast.py']:\n",
        "    filepath = deberta_v2_path/filename\n",
        "    if filepath.exists(): filepath.unlink()\n",
        "\n",
        "    shutil.copy(input_dir/filename, filepath)         "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWO81M6nUIpk",
        "outputId": "4a99c0bc-7db3-48c8-84d1-7bf37dd589cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: TOKENIZERS_PARALLELISM=true\n"
          ]
        }
      ],
      "source": [
        "import gc\n",
        "import ast\n",
        "import random\n",
        "from datetime import date, datetime\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing\n",
        "pd.set_option('display.max_rows', 500)\n",
        "pd.set_option('display.max_columns', 500)\n",
        "pd.set_option('display.width', 1300)\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast\n",
        "from transformers import RobertaTokenizerFast, AutoModel, AutoConfig\n",
        "from transformers import AdamW, get_scheduler\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "%env TOKENIZERS_PARALLELISM=true\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "from pynvml import * # for memory querying\n",
        "nvmlInit()\n",
        "H = nvmlDeviceGetHandleByIndex(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CdlOrv8_UIpm"
      },
      "outputs": [],
      "source": [
        "def seed_everything(seed=0):\n",
        "    '''\n",
        "    Sets the seed of the entire notebook so results are the same every time we run.\n",
        "    This is for REPRODUCIBILITY.\n",
        "    '''\n",
        "    random.seed(seed)\n",
        "    # Set a fixed value for the hash seed\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    \n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        # When running on the CuDNN backend, two further options must be set\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed_everything(seed=ModelParams.rand_seed)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "roberta_tokenizer = RobertaTokenizerFast.from_pretrained('roberta-large')\n",
        "deberta_tokenizer = DebertaV2TokenizerFast.from_pretrained('../input/deberta-tokenizer')\n",
        "# Note: offset mapping for these decoders are the same, but DeBERTa tokenizer decodes single tokens without a space, multiple tokens with a space between them\n",
        "\n",
        "if ModelParams.model_type == 'roberta':\n",
        "    cur_tokenizer = roberta_tokenizer\n",
        "\n",
        "    pt_folder = '../public_models/roberta_mlm'\n",
        "    if not os.path.exists(pt_folder): os.makedirs(pt_folder)\n",
        "    init_folder = '../public_models/roberta_initial'\n",
        "    if not os.path.exists(init_folder): os.makedirs(init_folder)\n",
        "    fin_folder = '../public_models/roberta_final'\n",
        "    if not os.path.exists(fin_folder): os.makedirs(fin_folder)\n",
        "elif ModelParams.model_type == 'deberta':\n",
        "    cur_tokenizer = deberta_tokenizer\n",
        "\n",
        "    pt_folder = '../public_models/deberta_mlm'\n",
        "    if not os.path.exists(pt_folder): os.makedirs(pt_folder)\n",
        "    init_folder = '../public_models/deberta_initial'\n",
        "    if not os.path.exists(init_folder): os.makedirs(init_folder)\n",
        "    fin_folder = '../public_models/deberta_final'\n",
        "    if not os.path.exists(fin_folder): os.makedirs(fin_folder)\n",
        "else: # public model\n",
        "    cur_tokenizer = deberta_tokenizer\n",
        "\n",
        "    init_folder = '../public_models/deberta-v3-large-5-folds-public'"
      ],
      "metadata": {
        "id": "m3iDCh_HRf0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KM0cx_fdUIpn"
      },
      "source": [
        "# Dataset creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQ8nzNZVUIpn"
      },
      "source": [
        "### Data helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FzHf4sW9UIpo"
      },
      "outputs": [],
      "source": [
        "def print_gpu_memory():\n",
        "    info = nvmlDeviceGetMemoryInfo(H)\n",
        "    print(f'total    : {info.total}')\n",
        "    print(f'free     : {info.free}')\n",
        "    print(f'used     : {info.used}')\n",
        "\n",
        "def write_to_file(file_path, print_string, option='a'):\n",
        "    # Need to write to file in this manner due to Colab's synchronization with Drive\n",
        "    f = open(file_path, option)\n",
        "    f.write(print_string)\n",
        "    f.close()\n",
        "    del f\n",
        "    gc.collect()\n",
        "\n",
        "def only_save_best_models(best_losses, epoch, loss, best_loss, fold_folder, model, optimizer=None):\n",
        "    '''\n",
        "    Function for managing data storage. Given the inputs, keep only the three best models\n",
        "    in storage in terms of the loss (train_loss or val_loss).\n",
        "\n",
        "    Returns: min(loss, best_loss)\n",
        "    '''\n",
        "    best_losses.append((epoch, loss))\n",
        "    best_losses.sort(key=lambda x: x[1])\n",
        "    epoch_folder = f\"{fold_folder}/epoch_{epoch+1}\"\n",
        "    # Only want to save three best models to save memory\n",
        "    if len(best_losses) > 3:\n",
        "        popped = best_losses.pop(3)\n",
        "        if popped[0] != epoch:\n",
        "            shutil.rmtree(f\"{fold_folder}/epoch_{popped[0]+1}\")\n",
        "            if not os.path.exists(epoch_folder): os.makedirs(epoch_folder)\n",
        "            torch.save(model.state_dict(), epoch_folder+\"/model.pth\")\n",
        "            if optimizer is not None: torch.save(optimizer.state_dict(), epoch_folder+\"/optim.pth\")\n",
        "    else:\n",
        "        if not os.path.exists(epoch_folder): os.makedirs(epoch_folder)\n",
        "        torch.save(model.state_dict(), epoch_folder+\"/model.pth\")\n",
        "        if optimizer is not None: torch.save(optimizer.state_dict(), epoch_folder+\"/optim.pth\")\n",
        "    return min(loss, best_loss)\n",
        "\n",
        "def create_k_fold_col(train_df):\n",
        "    skf = StratifiedKFold(n_splits=ModelParams.n_folds, shuffle=True, random_state=ModelParams.rand_seed)\n",
        "    if \"gold\" in train_df.columns:\n",
        "        train_df[\"stratify_on\"] = train_df[\"case_num\"].astype(str) + train_df[\"feature_num\"].astype(str) + train_df[\"gold\"].astype(str)\n",
        "    else:\n",
        "        train_df[\"stratify_on\"] = train_df[\"case_num\"].astype(str) + train_df[\"feature_num\"].astype(str)\n",
        "    train_df[\"fold\"] = -1      # Create a column to remember the fold\n",
        "    for fold, (_, valid_idx) in enumerate(skf.split(np.zeros(len(train_df)), y=train_df[\"stratify_on\"])):\n",
        "        train_df.loc[valid_idx, \"fold\"] = fold\n",
        "    return train_df\n",
        "\n",
        "def location_list(string_locs):\n",
        "    # example: ['0 1', '3 4', '4 4;5 6'] -> [(0, 1), (3, 4), (4, 4), (5, 6)]\n",
        "    loc_list = []\n",
        "    for loc_string in string_locs:\n",
        "        for section in loc_string.split(';'):\n",
        "            start, end = section.split()\n",
        "            loc_list.append((int(start), int(end)))\n",
        "    return loc_list\n",
        "\n",
        "def calculate_f1_metrics(predictions, references, attention_mask):\n",
        "    '''\n",
        "    Given a batch of predictions, references, and their attention_masks, return\n",
        "    the f1 metrics true_positive, false_positive, and false_negative from the batch.\n",
        "    \n",
        "    Args:\n",
        "        predictions (tensor - list of list of ints): Predictions for each token in each sentence of the batch\n",
        "        references (tensor - list of list of ints): Labels for each token in each sentence of the batch\n",
        "        attention_mask (tensor - list of list of ints): Attention mask for tokens in the batch\n",
        "\n",
        "    Returns:\n",
        "        true_positive, false_positive, false_negative: ints for each of these values (wrt desired label 1)\n",
        "    '''\n",
        "    predictions = predictions * attention_mask\n",
        "    references = references * attention_mask\n",
        "    true_positive = torch.sum(predictions*references) + 0.1\n",
        "    false_positive = torch.sum(predictions*(1-references)) + 0.1\n",
        "    false_negative = torch.sum((1-predictions)*references) + 0.1\n",
        "    return int(true_positive), int(false_positive), int(false_negative)\n",
        "\n",
        "def convert_to_labels(offset_mapping, loc_list):\n",
        "    '''\n",
        "    Given a bunch of offset_mappings, return a tensor with label 1 for each token\n",
        "    contained within the boundaries of a location in location_list.\n",
        "    \n",
        "    Args:\n",
        "        offset_mapping (list of tuples of two ints): Token spans.\n",
        "        loc_list (list of tuples of two ints): Char spans in sentence with positive feature label.\n",
        "\n",
        "    Returns:\n",
        "        torch tensor [ModelParams.max_length]: Binarized token label.\n",
        "    '''\n",
        "    labels = torch.zeros(ModelParams.max_length, dtype=torch.int64)\n",
        "    if len(loc_list) == 0: return labels\n",
        "    for i in range(ModelParams.max_length):\n",
        "        if offset_mapping[i][0] >= loc_list[0][0]-1 and offset_mapping[i][1] <= loc_list[-1][1]:\n",
        "            for loc in loc_list:\n",
        "                if offset_mapping[i][0] >= loc[0]-1:\n",
        "                    if offset_mapping[i][1] <= loc[1]:\n",
        "                        labels[i] = 1\n",
        "                else: break\n",
        "    return labels\n",
        "\n",
        "def get_char_probs(texts, predictions, offset_mappings):\n",
        "    '''\n",
        "    From the probabilities for individual tokens, generate probabilities for each\n",
        "    of the characters using the offset mappings.\n",
        "    '''\n",
        "    results = [np.zeros(len(t)) for t in texts]\n",
        "    for i, (prediction, offset_mapping) in enumerate(zip(predictions, offset_mappings)):\n",
        "        prediction = prediction[:len(offset_mapping)]\n",
        "        for idx, (pred, offset_tuple) in enumerate(zip(prediction, offset_mapping)):\n",
        "            start = offset_tuple[0]\n",
        "            end = offset_tuple[1]\n",
        "            results[i][start:end] = pred\n",
        "    return results\n",
        "\n",
        "def get_results(prediction, th=0.5):\n",
        "    '''\n",
        "    From the probabilities for individual characters, generate locations for words.\n",
        "    '''\n",
        "\n",
        "    def return_word_spans(text): # _Standard_ tokenization function for returning spans of individual words and punctuation\n",
        "        good_punctuation = set(';')\n",
        "        punctuation = set([',', '.', '?', '!', '/', ':', '-', '>', '<', '~', '[', ']', '_', ')', '*', '\\\\', '=', '(', '&', '#', '$', '@', '+', '\\n', '\\t'])\n",
        "        word_spans = []\n",
        "        prev_punc = -1\n",
        "        for i in range(len(text)):\n",
        "            if text[i] in punctuation:\n",
        "                if (i - prev_punc) != 1: word_spans.append((prev_punc+1, i))\n",
        "                word_spans.append((i, i+1))\n",
        "                prev_punc = i\n",
        "            elif text[i].isspace() or text[i] in good_punctuation:\n",
        "                if (i - prev_punc) != 1: word_spans.append((prev_punc+1, i))\n",
        "                prev_punc = i\n",
        "            elif text[i] == '\"':\n",
        "                if i+1 != len(text) and text[i+1] == '.': continue\n",
        "                if (i - prev_punc) != 1: word_spans.append((prev_punc+1, i))\n",
        "                prev_punc = i\n",
        "            elif text[i] == \"'\" and not (i-1 >= 0 and text[i-1].isalpha() and i+1 < len(text) and text[i+1].isalpha()):\n",
        "                if (i - prev_punc) != 1: word_spans.append((prev_punc+1, i))\n",
        "                word_spans.append((i, i+1))\n",
        "                prev_punc = i\n",
        "        return word_spans\n",
        "\n",
        "    def merge_spans(predictions, word_spans): # allows word spans to include spaces by merging consecutive word spans that are both inside the label\n",
        "        results = []\n",
        "        prev_word_inside = False\n",
        "        for word in word_spans:\n",
        "            label_prob = np.mean(predictions[word[0]:word[1]])\n",
        "            if label_prob >= th: \n",
        "                if prev_word_inside: results.append((results.pop()[0], word[1]))\n",
        "                else: results.append(word)\n",
        "                prev_word_inside = True\n",
        "            else: prev_word_inside = False\n",
        "        results = [f\"{tup[0]} {tup[1]}\" for tup in results]\n",
        "        results = \";\".join(results)\n",
        "        return results\n",
        "\n",
        "    prediction[\"word_spans\"] = prediction[\"text\"].apply(return_word_spans)\n",
        "    prediction[\"location\"] = prediction.apply(lambda x: merge_spans(x[\"predictions\"], x[\"word_spans\"]), axis=1)\n",
        "    prediction[\"id\"] =  prediction.apply(lambda x: str(x[\"pn_num\"]).rjust(5, '0') + '_' + str(x[\"feature_num\"]).rjust(3, '0'), axis=1)\n",
        "    prediction = prediction[['id', 'location']]\n",
        "\n",
        "    return prediction\n",
        "\n",
        "def get_char_locations(x, text):\n",
        "    # Gold character labels as a numpy array (len of text)\n",
        "    actual_locations = np.zeros(len(text))\n",
        "    for offset_tuple in location_list(x):\n",
        "        start = offset_tuple[0]\n",
        "        end = offset_tuple[1]\n",
        "        actual_locations[i][start:end] = 1\n",
        "    return actual_locations\n",
        "\n",
        "def produce_same_len_predictions(model_type, dataset, training_section=\"initial\"):\n",
        "    # Produce similar length predictions as public Kaggle model has a different max_length\n",
        "    prefix = \"\"\n",
        "    if dataset == \"train\": prefix = \"train_\"\n",
        "\n",
        "    if 'kaggle' not in model_type:\n",
        "        prediction_df = pd.read_csv(f\"../public_models/{model_type}_{training_section}/{prefix}predictions.csv\")\n",
        "        prediction_df[\"predictions\"] = prediction_df[\"predictions\"].apply(lambda x: np.array([float(i) for i in x[1:-1].split()]))\n",
        "        max_len_preds = np.array(prediction_df[\"predictions\"])\n",
        "    else:\n",
        "        prediction_df = pd.read_csv(f\"../public_models/deberta-v3-large-5-folds-public/{prefix}predictions.csv\")\n",
        "        prediction_df[\"predictions\"] = prediction_df[\"predictions\"].apply(lambda x: np.array([float(i) for i in x[1:-1].split()]))\n",
        "        max_len_preds = list(prediction_df[\"predictions\"])\n",
        "        for index, row in enumerate(predictions[0]):\n",
        "            newlen = len(row) - len(max_len_preds[index])\n",
        "            max_len_preds[index] = np.concatenate([max_len_preds[index], np.ones(newlen)*0.335]) # 0.335 is ~ threshold value for Kaggle predictions\n",
        "    return max_len_preds, prediction_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4xLzOPNFOSt"
      },
      "source": [
        "### Dataset annotation error cleanup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_3skiQgbFTWu"
      },
      "outputs": [],
      "source": [
        "# This function just cleans up some of the clear annotation errors\n",
        "def clean_datasets(symptoms, pn_notes, train_df):\n",
        "    symptoms.loc[27, 'feature_text'] = \"last pap smear 1 year ago\"\n",
        "\n",
        "    train_df.loc[[338], 'location'] = '[\"764 783\"]'\n",
        "    train_df.loc[[621], 'location'] = '[\"77 100\", \"398 420\"]'\n",
        "    train_df.loc[[655], 'location'] = '[\"285 292;301 312\", \"285 287;296 312\"]'\n",
        "    train_df.loc[[1262], 'location'] = '[\"551 557;565 580\"]'\n",
        "    train_df.loc[[1265], 'location'] = '[\"131 135;181 212\"]'\n",
        "    train_df.loc[[1424], 'location'] = '[\"74 88;109 129\"]'\n",
        "    train_df.loc[[1591], 'location'] = '[\"176 184;201 212\"]'\n",
        "    train_df.loc[[1615], 'location'] = '[\"249 257;271 288\"]'\n",
        "    train_df.loc[[1664], 'location'] = '[\"822 824;907 924\"]'\n",
        "    train_df.loc[[1714], 'location'] = '[\"101 129\"]'\n",
        "    train_df.loc[[1929], 'location'] = '[\"531 539;549 561\"]'\n",
        "    train_df.loc[[2134], 'location'] = '[\"540 560;581 593\"]'\n",
        "    train_df.loc[[2191], 'location'] = '[\"32 57\"]'\n",
        "    train_df.loc[[2553], 'location'] = '[\"308 317;376 384\"]'\n",
        "    train_df.loc[[3124], 'location'] = '[\"549 557\"]'\n",
        "    train_df.loc[[3858], 'location'] = '[\"102 123\", \"102 112;125 141\", \"102 112;143 157\", \"102 112;159 171\"]'\n",
        "    train_df.loc[[4373], 'location'] = '[\"33 45\"]'\n",
        "    train_df.loc[[4763], 'location'] = '[\"5 16\"]'\n",
        "    train_df.loc[[4782], 'location'] = '[\"175 194\"]'\n",
        "    train_df.loc[[4908], 'location'] = '[\"700 723\"]'\n",
        "    train_df.loc[[6016], 'location'] = '[\"225 250\"]'\n",
        "    train_df.loc[[6192], 'location'] = '[\"46 69\", \"197 218;236 260\"]'\n",
        "    train_df.loc[[6380], 'location'] = '[\"480 482;507 519\", \"480 482;499 503;512 519\", \"480 482;521 531\", \"480 482;533 545\", \"480 482;564 582\"]'\n",
        "    train_df.loc[[6562], 'location'] = '[\"290 320;327 337\", \"290 320;342 358\"]'\n",
        "    train_df.loc[[6862], 'location'] = '[\"288 296;324 363\"]'\n",
        "    train_df.loc[[7022], 'location'] = '[\"133 182\"]'\n",
        "    train_df.loc[[7422], 'location'] = '[\"102 125\"]'\n",
        "    train_df.loc[[8876], 'location'] = '[\"481 483;533 552\"]'\n",
        "    train_df.loc[[9027], 'location'] = '[\"92 102\", \"123 164\"]'\n",
        "    train_df.loc[[9875], 'location'] = '[\"151 156\", \"163 171\", \"221 225\"]'\n",
        "    train_df.loc[[9938], 'location'] = '[\"89 117\", \"122 138\"]'\n",
        "    train_df.loc[[9973], 'location'] = '[\"344 361\"]'\n",
        "    train_df.loc[[10513], 'location'] = '[\"600 623\"]'\n",
        "    train_df.loc[[11551], 'location'] = '[\"386 400;433 461\"]'\n",
        "    train_df.loc[[11677], 'location'] = '[\"160 201\"]'\n",
        "    train_df.loc[[12124], 'location'] = '[\"325 337;349 366\"]'\n",
        "    train_df.loc[[12279], 'location'] = '[\"405 459;488 524\"]'\n",
        "    train_df.loc[[12289], 'location'] = '[\"353 400;488 524\"]'\n",
        "    train_df.loc[[13238], 'location'] = '[\"293 307\", \"321 331\"]'\n",
        "    train_df.loc[[13297], 'location'] = '[\"182 221\", \"182 213;225 234\"]'\n",
        "    train_df.loc[[13299], 'location'] = '[\"71 96\"]'\n",
        "    train_df.loc[[13845], 'location'] = '[\"86 94;230 256\", \"86 94;237 256\"]'\n",
        "    train_df.loc[[14083], 'location'] = '[\"56 64;156 179\"]'\n",
        "\n",
        "    return symptoms, pn_notes, train_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1nb1ih3J3cM"
      },
      "source": [
        "### Data loading and torch dataset creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcYnEvP9UIpp",
        "outputId": "9c0aca5e-0362-4ef8-cddd-d7b11fa8d308"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   feature_num  case_num                                       feature_text\n",
            "0            0         0  Family-history-of-MI-OR-Family-history-of-myoc...\n",
            "1            1         0                 Family-history-of-thyroid-disorder\n",
            "   pn_num  case_num                                         pn_history\n",
            "0       0         0  17-year-old male, has come to the student heal...\n",
            "1       1         0  17 yo male with recurrent palpitations for the...\n",
            "          id  case_num  pn_num  feature_num                        annotation     location\n",
            "0  00016_000         0      16            0  ['dad with recent heart attcak']  ['696 724']\n",
            "1  00016_001         0      16            1     ['mom with \"thyroid disease']  ['668 693']\n"
          ]
        }
      ],
      "source": [
        "USE_SMALL_DEBUG_DATASET = False\n",
        "\n",
        "# Data directory\n",
        "DATA_DIR = '../input/nbme-score-clinical-patient-notes/'\n",
        "\n",
        "symptoms = pd.read_csv(DATA_DIR + 'features.csv')\n",
        "pn_notes = pd.read_csv(DATA_DIR + 'patient_notes.csv')\n",
        "train_df = pd.read_csv(DATA_DIR + 'train.csv')\n",
        "\n",
        "# An idea of the datasets\n",
        "print(symptoms.loc[0:1, :])\n",
        "print(pn_notes.loc[0:1, :])\n",
        "print(train_df.loc[0:1, :])\n",
        "\n",
        "train_df.drop(['id', 'annotation'], axis=1, inplace=True)\n",
        "\n",
        "symptoms['feature_text'] = symptoms['feature_text'].str.replace('-', ' ').str.replace(\" OR \", \"; \")\n",
        "symptoms[\"feature_text\"] = symptoms[\"feature_text\"].apply(lambda x: x.lower())\n",
        "pn_notes[\"pn_history\"] = pn_notes[\"pn_history\"].apply(lambda x: x.lower())\n",
        "\n",
        "# Clean up some clear annotation errors\n",
        "symptoms, pn_notes, train_df = clean_datasets(symptoms, pn_notes, train_df)\n",
        "\n",
        "train_df['location'] = train_df['location'].apply(lambda x: ast.literal_eval(x))   # train_df['location'][index] has format \"[\"1 2\"]\"\n",
        "\n",
        "if USE_SMALL_DEBUG_DATASET:\n",
        "    _, train_df = train_test_split(train_df, test_size=0.01, random_state=42)\n",
        "    symptoms = symptoms.merge(train_df, how='inner', on=['case_num', 'feature_num'])\n",
        "    symptoms.drop(columns=['location', 'pn_num'], inplace=True)\n",
        "    symptoms = symptoms.groupby(['case_num', 'feature_num'], as_index=False).first()\n",
        "    pn_notes = pn_notes.merge(train_df, how='inner', on=['case_num', 'pn_num'])\n",
        "    pn_notes.drop(columns=['location', 'feature_num'], inplace=True)\n",
        "    pn_notes = pn_notes.groupby(['case_num', 'pn_num'], as_index=False).first()\n",
        "    ModelParams.n_folds = 3\n",
        "    train_df.reset_index(inplace=True, drop=True)\n",
        "    symptoms.reset_index(inplace=True, drop=True)\n",
        "    pn_notes.reset_index(inplace=True, drop=True)\n",
        "    print(len(train_df))\n",
        "    print(len(symptoms))\n",
        "    print(len(pn_notes))\n",
        "\n",
        "train_dataset = symptoms.merge(pn_notes, how='outer', on=['case_num'])\n",
        "\n",
        "# Cross entropy weights, making use of the natural balance between the ones and\n",
        "# the zeros in the dataset of 69.4.\n",
        "ce_weights = torch.tensor([1, torch.log(torch.tensor(69.4))], device=device)\n",
        "\n",
        "# Dataset class for token-wise labeled data creation\n",
        "class PatientNotesDataset(Dataset):\n",
        "    \n",
        "    def __init__(self, train_dataset, labels, tokenizer):\n",
        "        self.data = train_dataset.merge(labels, how='inner', on=['case_num', 'pn_num', 'feature_num'])\n",
        "        self.data.reset_index(inplace=True, drop=True)\n",
        "        self.tokenizer = tokenizer\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        tokenized_text = self.tokenizer(self.data['pn_history'][index],\n",
        "                                        self.data['feature_text'][index],\n",
        "                                           max_length=ModelParams.max_length,\n",
        "                                           padding='max_length',\n",
        "                                           return_offsets_mapping=True,\n",
        "                                           return_tensors='pt')\n",
        "        \n",
        "        # returned outputs from tokenizer have batch size 1, need to reshape\n",
        "        for k in tokenized_text.keys():\n",
        "            tokenized_text[k] = tokenized_text[k].view(*tokenized_text[k].shape[1:])\n",
        "        \n",
        "        # Want to zero out the second sentence\n",
        "        sep_tok_index = ModelParams.max_length-1\n",
        "        for i, v in enumerate(tokenized_text['input_ids']):\n",
        "            if v == self.tokenizer.sep_token_id:\n",
        "                sep_tok_index = i\n",
        "                break\n",
        "        tokenized_text['attention_mask'][sep_tok_index+1:] = 0\n",
        "        \n",
        "        # Convert location to labels for tokens\n",
        "        loc_list = location_list(self.data['location'][index])\n",
        "        labels = convert_to_labels(tokenized_text['offset_mapping'], loc_list)\n",
        "        \n",
        "        return {'input_ids': tokenized_text['input_ids'], \\\n",
        "                'attention_mask': tokenized_text['attention_mask'].type(torch.int8), \\\n",
        "                'labels': labels}\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "# Model for token-wise classification\n",
        "class TokenModel(nn.Module):\n",
        "\n",
        "    def __init__(self, cfg, config_path=None):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.config = AutoConfig.from_pretrained(cfg, output_hidden_states=True)\n",
        "        if config_path is not None:\n",
        "            self.model = AutoModel.from_pretrained(config_path, config=self.config)\n",
        "        else:\n",
        "            self.model = AutoModel.from_config(self.config)\n",
        "\n",
        "        if \"roberta\" in cfg:\n",
        "            for param in self.model.parameters():\n",
        "                param.requires_grad = False\n",
        "        elif \"deberta\" in cfg:\n",
        "            for param in self.model.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "        self.proj_size = 84   # hidden size between 2 linear layers\n",
        "        self.fc1 = nn.Linear(self.config.hidden_size, self.proj_size)\n",
        "        self.fc2 = nn.Linear(self.proj_size, 2)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "    def feature(self, inputs):\n",
        "        outputs = self.model(**inputs)\n",
        "        last_hidden_states = outputs[0]\n",
        "        \n",
        "        return last_hidden_states\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        feature = self.feature(inputs)\n",
        "        hidden = self.relu(self.fc1(self.dropout(feature)))\n",
        "        output = self.fc2(self.dropout(hidden))\n",
        "        output.transpose_(1,2)\n",
        "        \n",
        "        return output\n",
        "\n",
        "\n",
        "# Model layout from Kaggle\n",
        "class ScoringModel(nn.Module):\n",
        "    def __init__(self, config_path=None, pretrained=False):\n",
        "        super().__init__()\n",
        "        \n",
        "        if config_path is None:\n",
        "            self.config = AutoConfig.from_pretrained(\"microsoft/deberta-v3-large\", output_hidden_states=True)\n",
        "        else:\n",
        "            self.config = torch.load(config_path)\n",
        "        if pretrained:\n",
        "            self.model = AutoModel.from_pretrained(\"microsoft/deberta-v3-large\", config=self.config)\n",
        "        else:\n",
        "            self.model = AutoModel.from_config(self.config)\n",
        "        self.fc_dropout = nn.Dropout(0.2)\n",
        "        self.fc = nn.Linear(self.config.hidden_size, 1)\n",
        "                \n",
        "    def feature(self, inputs):\n",
        "        outputs = self.model(**inputs)\n",
        "        last_hidden_states = outputs[0]\n",
        "        \n",
        "        return last_hidden_states\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        feature = self.feature(inputs)\n",
        "        output = self.fc(self.fc_dropout(feature))\n",
        "        \n",
        "        return output\n",
        "\n",
        "\n",
        "# Dataset class for test-set creation\n",
        "class TestDataset(Dataset):\n",
        "    \n",
        "    def __init__(self, train_dataset, test_samples, tokenizer, public_kaggle_model=False):\n",
        "        self.data = train_dataset.merge(test_samples, how='inner', on=['case_num', 'pn_num', 'feature_num'])\n",
        "        self.data.reset_index(inplace=True, drop=True)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.public_kaggle_model = public_kaggle_model\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        pn_note = self.data['pn_history'][index]\n",
        "\n",
        "        if self.public_kaggle_model: length = 354\n",
        "        else: length = ModelParams.max_length\n",
        "\n",
        "        tokenized_text = self.tokenizer(pn_note, self.data['feature_text'][index],\n",
        "                                           max_length=length,\n",
        "                                           padding='max_length',\n",
        "                                           return_offsets_mapping=True,\n",
        "                                           return_tensors='pt')\n",
        "        \n",
        "        # returned outputs from tokenizer have batch size 1, need to reshape\n",
        "        for k in tokenized_text.keys():\n",
        "            tokenized_text[k] = tokenized_text[k].view(*tokenized_text[k].shape[1:])\n",
        "        \n",
        "        # Want to zero out the second sentence\n",
        "        sep_tok_index = ModelParams.max_length-1\n",
        "        for i, v in enumerate(tokenized_text['input_ids']):\n",
        "            if v == self.tokenizer.sep_token_id:\n",
        "                sep_tok_index = i\n",
        "                break\n",
        "        tokenized_text['attention_mask'][sep_tok_index+1:] = 0\n",
        "        tokenized_text['offset_mapping'] = tokenized_text['offset_mapping'][:sep_tok_index+1]\n",
        "        \n",
        "        return {'input_ids': tokenized_text['input_ids'], \\\n",
        "                'attention_mask': tokenized_text['attention_mask'].type(torch.int8), \\\n",
        "                'offset_mapping': tokenized_text['offset_mapping'].type(torch.int16), \\\n",
        "                'text': pn_note}\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "# print([i for i in list(globals().keys()) if len(i) > 0 and i[0] != '_'])\n",
        "# del globals()['symptoms'], globals()['pn_notes']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cdy4C1IRUIpq"
      },
      "source": [
        "# Pre-training our Language Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "ai2J5adEUIpq",
        "outputId": "1253d5d0-9d32-4351-f0ae-a2032d9aff11"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nAfter running the below block of code, I realized that 1e-5 is the best learning\\nrate in my opinion, and moreover the training loss as calculated below is a\\nvery close approximation to validation loss. I suppose this is because the task\\nis mlm, and we're predicting so many embeddings that the gradients are averaged out\\nand training loss is a close approximation to validation loss. So we don't need the below\\ncode.\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "from transformers import RobertaForMaskedLM, DebertaV2ForMaskedLM\n",
        "import random\n",
        "\n",
        "class MLMPreTrainDataset(Dataset):\n",
        "\n",
        "    def __init__(self, pn_notes, symptoms, tokenizer):\n",
        "        pn_notes_examples = tokenizer(list(pn_notes['pn_history']),\n",
        "                                           add_special_tokens=True,\n",
        "                                           max_length=ModelParams.max_length,\n",
        "                                           padding='max_length')\n",
        "        \n",
        "        self.input_ids = list(pn_notes_examples['input_ids'])\n",
        "        \n",
        "        symptoms_examples = tokenizer(list(symptoms['feature_text']),\n",
        "                                           add_special_tokens=True,\n",
        "                                           max_length=ModelParams.max_length,\n",
        "                                           padding='max_length')\n",
        "\n",
        "        for i in range(3):\n",
        "            self.input_ids += list(symptoms_examples['input_ids'])\n",
        "\n",
        "        self.tokenizer=tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        tokens = self.input_ids[index]\n",
        "        rand = torch.rand(len(tokens))\n",
        "        attention_mask = (rand < 0.15) * (tokens!=self.tokenizer.cls_token_id) * (tokens!=self.tokenizer.sep_token_id) * (tokens!=self.tokenizer.pad_token_id)\n",
        "        attention_mask = attention_mask.type(torch.int8)\n",
        "        labels = torch.zeros(len(tokens), dtype=torch.int64)\n",
        "\n",
        "        for i, (token) in enumerate(tokens):\n",
        "            prob = rand[i]\n",
        "            if prob < 0.15:\n",
        "                prob /= 0.15\n",
        "\n",
        "                # 80% randomly change token to mask token\n",
        "                if prob < 0.8: labels[i] = self.tokenizer.mask_token_id\n",
        "\n",
        "                # 10% randomly change token to random token\n",
        "                elif prob < 0.9:\n",
        "                    line = [i for i in self.input_ids[random.randrange(len(self.input_ids))] if i != 0]\n",
        "                    labels[i] = line[random.randrange(len(line))]\n",
        "\n",
        "                # 10% randomly change token to current token, i.e. do nothing\n",
        "                else: labels[i] = tokens[i]\n",
        "\n",
        "        return {'input_ids': torch.tensor(tokens, dtype=torch.int64), 'attention_mask': attention_mask, 'labels': labels}\n",
        "\n",
        "'''\n",
        "After running the below block of code, I realized that 1e-5 is the best learning\n",
        "rate in my opinion, and moreover the training loss as calculated below is a\n",
        "very close approximation to validation loss. I suppose this is because the task\n",
        "is mlm, and we're predicting so many embeddings that the gradients are averaged out\n",
        "and training loss is a close approximation to validation loss. So we don't need the below\n",
        "code.\n",
        "'''\n",
        "\n",
        "# # Train-test split for evaluation\n",
        "# symptoms_train, symptoms_eval = train_test_split(symptoms, test_size=0.15, random_state=ModelParams.rand_seed)\n",
        "# pn_notes_train, pn_notes_eval = train_test_split(pn_notes, test_size=0.15, random_state=ModelParams.rand_seed)\n",
        "\n",
        "# train_metrics_path = f\"{pt_folder}/training_metrics.txt\"\n",
        "\n",
        "# # Test out a number of different learning rates\n",
        "# for lr in [1e-5, 2.5e-5, 5e-5]:\n",
        "#     if ModelParams.model_type == 'roberta':\n",
        "#         pt_model = RobertaForMaskedLM.from_pretrained('roberta-large')\n",
        "#     elif ModelParams.model_type == 'deberta':\n",
        "#         pt_model = DebertaV2ForMaskedLM.from_pretrained(\"microsoft/deberta-v3-large\")\n",
        "\n",
        "#     if lr == 5e-5: num_epochs = 3\n",
        "#     elif lr == 2.5e-5: num_epochs = 6\n",
        "#     else: num_epochs = 10\n",
        "\n",
        "#     # move the model to device\n",
        "#     pt_model.to(device)\n",
        "#     # Train Model\n",
        "#     pt_model.train()\n",
        "\n",
        "#     # Create the train and evaluation dataset for mlm\n",
        "#     pt_train_dataset = MLMPreTrainDataset(pn_notes_train, symptoms_train, cur_tokenizer)\n",
        "#     pt_eval_dataset = MLMPreTrainDataset(pn_notes_eval, symptoms_eval, cur_tokenizer)\n",
        "\n",
        "#     pt_train_dataloader = DataLoader(dataset=pt_train_dataset, batch_size=ModelParams.pt_batch_size, shuffle=True, num_workers=ModelParams.num_workers, pin_memory=ModelParams.pin_mem)\n",
        "#     pt_eval_dataloader = DataLoader(dataset=pt_eval_dataset, batch_size=ModelParams.eval_batch_size, num_workers=ModelParams.num_workers, pin_memory=ModelParams.pin_mem)\n",
        "\n",
        "#     # Optimizer and schedules for optimizer\n",
        "#     pt_optimizer = AdamW(pt_model.parameters(), weight_decay=0.01)\n",
        "\n",
        "#     num_training_steps = num_epochs * len(pt_train_dataloader)\n",
        "#     num_steps = 0\n",
        "#     num_warmup_steps = num_training_steps*0.01\n",
        "#     print(f\"Num warmup steps: {num_warmup_steps}\")\n",
        "\n",
        "#     print(f\"Running learning rate of {lr} for {num_epochs} epochs.\")\n",
        "#     progress_bar = tqdm(range(num_training_steps))\n",
        "\n",
        "#     row_format =\"{:<10} {:<20} {:<15}\"\n",
        "#     now = datetime.now()\n",
        "#     dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
        "#     write_to_file(train_metrics_path, f\"Run on {dt_string}. LR = {lr}, Epochs = {num_epochs}\\n\")\t\n",
        "#     write_to_file(train_metrics_path, row_format.format(\"\", \"Approx. train loss\", \"Val loss\") + '\\n')\n",
        "\n",
        "#     val_losses = []\n",
        "\n",
        "#     for epoch in range(num_epochs):\n",
        "#         pt_optimizer.param_groups[0]['lr'] = pt_optimizer.param_groups[0]['lr']*0.65\n",
        "#         for pt_batch in pt_train_dataloader:\n",
        "#             if num_steps < num_warmup_steps:\n",
        "#                 pt_optimizer.param_groups[0]['lr'] = lr*(num_steps+1)/num_warmup_steps\n",
        "#                 num_steps+=1\n",
        "\n",
        "            # num_in_batch = len(pt_batch['input_ids'])\n",
        "#             num_done = 0\n",
        "#             while num_done < num_in_batch:      # need to use sub_batches due to RAM\n",
        "#                 num_todo = min(ModelParams.sub_batch_size, num_in_batch-num_done)\n",
        "#                 sub_pt_batch = {k: v[num_done:num_done+num_todo].to(device, non_blocking=True) for k, v in pt_batch.items()}\n",
        "#                 sub_loss = pt_model(**sub_pt_batch).loss*num_todo/num_in_batch\n",
        "#                 sub_loss.backward()\n",
        "#                 del globals()[\"sub_loss\"], globals()[\"sub_pt_batch\"]\n",
        "#                 torch.cuda.empty_cache()\n",
        "#                 num_done+=num_todo\n",
        "            \n",
        "#             pt_optimizer.step()\n",
        "#             pt_optimizer.zero_grad(set_to_none=True)\n",
        "#             progress_bar.update(1)\n",
        "\n",
        "#         # Every epoch print metrics, save if necessary\n",
        "#         pt_model.eval()\n",
        "#         with torch.no_grad():\n",
        "#             # compute train loss\n",
        "#             train_loss = 0\n",
        "#             rand_loss_iters = set(random.sample(range(len(pt_train_dataloader)), len(pt_train_dataloader)//10)) # eval 0.1 of training set for approx.\n",
        "#             count = -1\n",
        "#             for pt_batch in pt_train_dataloader:\n",
        "                # count += 1\n",
        "                # if count not in rand_loss_iters: continue\n",
        "\n",
        "                # pt_batch = {k: v.to(device, non_blocking=True) for k, v in pt_batch.items()}\n",
        "                # sub_loss = pt_model(**pt_batch).loss.item()*ModelParams.eval_batch_size/len(rand_loss_iters)\n",
        "                # train_loss += sub_loss\n",
        "\n",
        "#             # compute val loss and metrics\n",
        "#             val_loss = 0\n",
        "#             for pt_batch in pt_eval_dataloader:\n",
        "#                 pt_batch = {k: v.to(device, non_blocking=True) for k, v in pt_batch.items()}\n",
        "#                 sub_loss = pt_model(**pt_batch).loss.item()*ModelParams.eval_batch_size/len(pt_eval_dataloader)\n",
        "#                 val_loss += sub_loss\n",
        "#             del globals()[\"pt_batch\"]\n",
        "#             torch.cuda.empty_cache()\n",
        "\n",
        "#             write_to_file(train_metrics_path, row_format.format(f\"Epoch {epoch+1}\", f\"{train_loss:.6f}\", f\"{val_loss:.6f}\") + '\\n')\n",
        "\n",
        "#             val_losses.append(val_loss)\n",
        "#             if epoch >= 3 and val_loss > (val_losses[epoch-3] + val_losses[epoch-2]*0.9 + val_losses[epoch-1]*0.9**2)/(1+0.9+0.9**2):\n",
        "#                 write_to_file(train_metrics_path, \"\\nValidation loss stopped decreasing compared to weighted average of last three epochs, stopping.\\n\")\n",
        "#                 break\n",
        "#         pt_model.train()\n",
        "\n",
        "#     keys = list(globals().keys())\n",
        "#     for k in keys:\n",
        "#         if k.startswith('pt_'): del globals()[k]\n",
        "#     gc.collect()\n",
        "#     torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sT-qlw_O7To"
      },
      "source": [
        "After determining the best learning rate, train the model on the entire corpus:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGxKQcVePGaQ"
      },
      "outputs": [],
      "source": [
        "if ModelParams.model_type == 'roberta':\n",
        "    pt_model = RobertaForMaskedLM.from_pretrained('roberta-large')\n",
        "elif ModelParams.model_type == 'deberta':\n",
        "    pt_model = DebertaV2ForMaskedLM.from_pretrained(\"microsoft/deberta-v3-large\")\n",
        "\n",
        "best_epochs = 6\n",
        "best_lr = 1.6e-5\n",
        "\n",
        "# move the model to device\n",
        "pt_model.to(device)\n",
        "# Train Model\n",
        "pt_model.train()\n",
        "\n",
        "# Create the train and evaluation dataset for mlm\n",
        "pt_train_dataset = MLMPreTrainDataset(pn_notes, symptoms, cur_tokenizer)\n",
        "pt_train_dataloader = DataLoader(dataset=pt_train_dataset, batch_size=ModelParams.pt_batch_size, shuffle=True, num_workers=ModelParams.num_workers, pin_memory=ModelParams.pin_mem)\n",
        "pt_train_eval_dataloader = DataLoader(dataset=pt_train_dataset, batch_size=ModelParams.eval_batch_size, num_workers=ModelParams.num_workers, pin_memory=ModelParams.pin_mem)\n",
        "\n",
        "# Optimizer and schedules for optimizer\n",
        "pt_optimizer = AdamW(pt_model.parameters(), weight_decay=0.01)\n",
        "\n",
        "num_training_steps = best_epochs * len(pt_train_dataloader)\n",
        "num_steps = 0\n",
        "num_warmup_steps = num_training_steps*0.01\n",
        "print(f\"Num warmup steps: {num_warmup_steps}\")\n",
        "\n",
        "train_metrics_path = f\"{pt_folder}/training_metrics.txt\"\n",
        "\n",
        "print(f\"Running learning rate of {best_lr} for {best_epochs} epochs.\")\n",
        "progress_bar = tqdm(range(num_training_steps))\n",
        "row_format =\"{:<10} {:<20}\"\n",
        "now = datetime.now()\n",
        "dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
        "write_to_file(train_metrics_path, f\"Run on {dt_string}. LR = {best_lr}, Epochs = {best_epochs}\\n\")\n",
        "write_to_file(train_metrics_path, row_format.format(\"\", \"Approx. train loss\") + '\\n')\n",
        "\n",
        "best_train_losses = []\n",
        "train_losses = []\n",
        "best_train_loss = np.inf\n",
        "\n",
        "for epoch in range(best_epochs):\n",
        "    pt_optimizer.param_groups[0]['lr'] = pt_optimizer.param_groups[0]['lr']*0.65\n",
        "    for pt_batch in pt_train_dataloader:\n",
        "        if num_steps < num_warmup_steps:\n",
        "            pt_optimizer.param_groups[0]['lr'] = best_lr*(num_steps+1)/num_warmup_steps\n",
        "            num_steps+=1\n",
        "\n",
        "        num_in_batch = len(pt_batch['input_ids'])\n",
        "        num_done = 0\n",
        "        while num_done < num_in_batch:      # need to use sub_batches due to RAM\n",
        "            num_todo = min(ModelParams.sub_batch_size, num_in_batch-num_done)\n",
        "            sub_pt_batch = {k: v[num_done:num_done+num_todo].to(device, non_blocking=True) for k, v in pt_batch.items()}\n",
        "            sub_loss = pt_model(**sub_pt_batch).loss*num_todo/num_in_batch\n",
        "            sub_loss.backward()\n",
        "\n",
        "            del globals()[\"sub_loss\"], globals()[\"sub_pt_batch\"]\n",
        "            torch.cuda.empty_cache()\n",
        "            num_done+=num_todo\n",
        "        \n",
        "        pt_optimizer.step()\n",
        "        pt_optimizer.zero_grad(set_to_none=True)\n",
        "        progress_bar.update(1)\n",
        "\n",
        "    # Every epoch print metrics, save if necessary\n",
        "    pt_model.eval()\n",
        "    with torch.no_grad():\n",
        "        # compute train loss\n",
        "        train_loss = 0\n",
        "        rand_loss_iters = set(random.sample(range(len(pt_train_eval_dataloader)), len(pt_train_eval_dataloader)//10)) # eval 0.1 of training set for approx.\n",
        "        count = -1\n",
        "\n",
        "        for pt_batch in pt_train_eval_dataloader:\n",
        "            count += 1\n",
        "            if count not in rand_loss_iters: continue\n",
        "\n",
        "            pt_batch = {k: v.to(device, non_blocking=True) for k, v in pt_batch.items()}\n",
        "            sub_loss = pt_model(**pt_batch).loss.item()*ModelParams.eval_batch_size/len(rand_loss_iters)\n",
        "            train_loss += sub_loss\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        write_to_file(train_metrics_path, row_format.format(f\"Epoch {epoch+1}\", f\"{train_loss:.6f}\") + '\\n')\n",
        "\n",
        "        if epoch >= 1 and train_loss < train_losses[epoch-1] and train_loss < 1.05*best_train_loss:\n",
        "            best_train_loss = only_save_best_models(best_train_losses, epoch, train_loss, best_train_loss, pt_folder, pt_model)\n",
        "    pt_model.train()\n",
        "\n",
        "keys = list(globals().keys())\n",
        "for k in keys:\n",
        "    if k.startswith('pt_'): del globals()[k]\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After determining the best number of epochs, make a special folder for that epoch."
      ],
      "metadata": {
        "id": "RdxAncYKqvfL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_epoch = 3\n",
        "epoch_folder = f\"{pt_folder}/epoch_{best_epoch}\"\n",
        "del_epochs = [folder.name for folder in os.scandir(pt_folder) if (folder.is_dir() and \"second\" not in folder.name)]\n",
        "del_epochs.remove(f\"epoch_{best_epoch}\")\n",
        "for del_epoch in del_epochs:\n",
        "    os.renames(f\"{pt_folder}/{del_epoch}\", f\"{pt_folder}/second_best_epochs/{del_epoch}\")\n",
        "\n",
        "os.renames(f\"{pt_folder}/epoch_{best_epoch}\", f\"{pt_folder}/best_epoch\")"
      ],
      "metadata": {
        "id": "4_inqfvGqf29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xN9Ju_wQC2r"
      },
      "source": [
        "# Fine-tuning our model on the small training set\n",
        "\n",
        "(and applying the self supervised labels to the rest of the symptom notes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4HYRNgs0QWf5"
      },
      "outputs": [],
      "source": [
        "train_df = create_k_fold_col(train_df)\n",
        "\n",
        "graph_train_val = {}        # for graphing train loss/val loss\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss(ce_weights, reduction='sum')\n",
        "\n",
        "for fold in [1]:#, 1, 2, 4] or range(ModelParams.num_epochs)\n",
        "    if ModelParams.model_type == 'roberta':\n",
        "        init_model = TokenModel('roberta-large', f\"{pt_folder}/best_epoch/model.pth\")\n",
        "    elif ModelParams.model_type == 'deberta':\n",
        "        init_model = TokenModel('microsoft/deberta-v3-large', f\"{pt_folder}/best_epoch/model.pth\")\n",
        "\n",
        "    # move the model to device\n",
        "    init_model.to(device)\n",
        "    # Train Model\n",
        "    init_model.train()\n",
        "\n",
        "    # Output folder for model weights\n",
        "    fold_folder = f\"{init_folder}/fold_{fold}\"\n",
        "    if not os.path.exists(fold_folder): os.makedirs(fold_folder)\n",
        "\n",
        "    train_metrics_path = f\"{fold_folder}/training_metrics.txt\"\n",
        "    graph_train_val[fold] = {'train_loss': [], 'val_loss': [], 'train_f1': [], 'val_f1': []}  # new list element for fold\n",
        "\n",
        "    init_train_df = train_df[train_df['fold'] != fold].copy(deep=True)\n",
        "    init_eval_df = train_df[train_df['fold'] == fold].copy(deep=True)\n",
        "    init_train_dataset = PatientNotesDataset(train_dataset, init_train_df, cur_tokenizer)\n",
        "    init_eval_dataset = PatientNotesDataset(train_dataset, init_eval_df, cur_tokenizer)\n",
        "    init_train_dataloader = DataLoader(dataset=init_train_dataset, batch_size=ModelParams.init_batch_size, shuffle=True, num_workers=ModelParams.num_workers, pin_memory=ModelParams.pin_mem)\n",
        "    init_train_eval_dataloader = DataLoader(dataset=init_train_dataset, batch_size=ModelParams.eval_batch_size, num_workers=ModelParams.num_workers, pin_memory=ModelParams.pin_mem)\n",
        "    init_eval_dataloader = DataLoader(dataset=init_eval_dataset, batch_size=ModelParams.eval_batch_size, num_workers=ModelParams.num_workers, pin_memory=ModelParams.pin_mem)\n",
        "\n",
        "    # Optimizer and schedules for optimizer\n",
        "    init_optimizer = AdamW(init_model.parameters(), weight_decay=0.01)\n",
        "\n",
        "    base_lr=3e-5      # Might need higher learning rate for here, SEEEEEEEEEEEEEEE\n",
        "    num_warmup_steps=1000\n",
        "\n",
        "    num_epochs = ModelParams.num_epochs\n",
        "    num_training_steps = num_epochs * len(init_train_dataloader)\n",
        "    num_steps = 0\n",
        "\n",
        "    best_val_loss = np.inf\n",
        "    best_val_losses = []\n",
        "\n",
        "    print(f\"Fold {fold}:\\n\")\n",
        "    progress_bar = tqdm(range(num_training_steps))\n",
        "    row_format =\"{:<10} {:<20} {:<15} {:<20} {:<15}\"\n",
        "    now = datetime.now()\n",
        "    dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
        "    write_to_file(train_metrics_path, f\"Run on {dt_string}. LR = {base_lr}, Epochs = {ModelParams.num_epochs}\\n\")\t\n",
        "    write_to_file(train_metrics_path, row_format.format(\"\", \"Approx. train loss\", \"Val loss\", \"Approx. train f1\", \"Val f1\") + '\\n')\n",
        "\n",
        "    for epoch in range(ModelParams.num_epochs):\n",
        "        init_optimizer.param_groups[0]['lr'] = init_optimizer.param_groups[0]['lr']*0.65\n",
        "        \n",
        "        for init_batch in init_train_dataloader:\n",
        "            if num_steps < num_warmup_steps:\n",
        "                init_optimizer.param_groups[0]['lr'] = base_lr*(num_steps+1)/num_warmup_steps\n",
        "                num_steps+=1\n",
        "\n",
        "            init_batch = {k: v.to(device, non_blocking=True) for k, v in init_batch.items()}\n",
        "            num_in_batch = len(init_batch['input_ids'])\n",
        "            labels = init_batch['labels']\n",
        "            del init_batch['labels']\n",
        "            outputs = init_model(init_batch)\n",
        "            loss = loss_fn(outputs, labels)/num_in_batch\n",
        "            loss.backward()\n",
        "\n",
        "            del init_batch, labels, outputs, loss\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "            init_optimizer.step()\n",
        "            init_optimizer.zero_grad(set_to_none=True)\n",
        "            progress_bar.update(1)\n",
        "\n",
        "        # Every epoch print metrics, save if necessary\n",
        "        init_model.eval()\n",
        "        with torch.no_grad():\n",
        "            train_tp, train_fp, train_fn = 0, 0, 0\n",
        "\n",
        "            # compute train loss\n",
        "            train_loss = 0\n",
        "            rand_loss_iters = set(random.sample(range(len(init_train_eval_dataloader)), len(init_train_eval_dataloader)//10)) # eval 0.1 of training set for approx.\n",
        "\n",
        "            for count, init_batch in enumerate(init_train_eval_dataloader):\n",
        "                if count not in rand_loss_iters: continue\n",
        "\n",
        "                init_batch = {k: v.to(device, non_blocking=True) for k, v in init_batch.items()}\n",
        "                labels = init_batch['labels']\n",
        "                del init_batch['labels']\n",
        "                outputs = init_model(init_batch)\n",
        "                train_loss += loss_fn(outputs, labels)/len(rand_loss_iters)\n",
        "\n",
        "                predictions = torch.argmax(outputs, dim=1) # Might need different way to find predictions depending on weight\n",
        "\n",
        "\n",
        "                f1_metrics = calculate_f1_metrics(predictions=predictions, references=labels, attention_mask=init_batch['attention_mask'])\n",
        "                train_tp += f1_metrics[0]\n",
        "                train_fp += f1_metrics[1]\n",
        "                train_fn += f1_metrics[2]\n",
        "\n",
        "            val_tp, val_fp, val_fn = 0, 0, 0\n",
        "\n",
        "            # compute val loss and metrics\n",
        "            val_loss = 0\n",
        "            for init_batch in init_eval_dataloader:\n",
        "                init_batch = {k: v.to(device, non_blocking=True) for k, v in init_batch.items()}\n",
        "                labels = init_batch['labels']\n",
        "                del init_batch['labels']\n",
        "\n",
        "                outputs = init_model(init_batch)\n",
        "                val_loss += loss_fn(outputs, labels)/len(init_eval_dataloader)\n",
        "\n",
        "                predictions = torch.argmax(outputs, dim=1)\n",
        "                f1_metrics = calculate_f1_metrics(predictions=predictions, references=labels, attention_mask=init_batch['attention_mask'])\n",
        "                val_tp += f1_metrics[0]\n",
        "                val_fp += f1_metrics[1]\n",
        "                val_fn += f1_metrics[2]\n",
        "\n",
        "            del init_batch, outputs, labels, predictions, f1_metrics\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "            if train_tp == 0 and train_fp == 0 and train_fn == 0: train_f1 = -1   # -1 as in weird input/error\n",
        "            else: train_f1 = train_tp / (train_tp + 0.5*(train_fp+train_fn))\n",
        "            if val_tp == 0 and val_fp == 0 and val_fn == 0: val_f1 = -1\n",
        "            else: val_f1 = val_tp / (val_tp + 0.5*(val_fp+val_fn))\n",
        "            print(row_format.format(f\"Epoch {epoch+1}\", f\"{train_loss:.6f}\", f\"{val_loss:.6f}\", f\"{train_f1:.6f}\", f\"{val_f1:.6f}\"))\n",
        "            write_to_file(train_metrics_path, row_format.format(f\"Epoch {epoch+1}\", f\"{train_loss:.6f}\", f\"{val_loss:.6f}\", f\"{train_f1:.6f}\", f\"{val_f1:.6f}\") + '\\n')\n",
        "\n",
        "            for metric in ['train_loss', 'val_loss', 'train_f1', 'val_f1']:\n",
        "                graph_train_val[fold][metric].append(float(locals()[metric]))\n",
        "\n",
        "            if epoch >= 7 and val_loss > (graph_train_val[fold]['val_loss'][epoch-3] + graph_train_val[fold]['val_loss'][epoch-2]*0.9 \\\n",
        "                                            + graph_train_val[fold]['val_loss'][epoch-1]*0.9**2)/(1+0.9+0.9**2):\n",
        "                write_to_file(train_metrics_path, \"\\nValidation loss stopped decreasing compared to weighted average of last three epochs, stopping.\\n\")\n",
        "                break\n",
        "\n",
        "            # good_val_loss = 1-train_f1+(val_loss-2.5)*3\n",
        "            # graph_train_val[fold]['val_loss'].append(good_val_loss)            \n",
        "            \n",
        "            if epoch >= 1 and val_loss < graph_train_val[fold]['val_loss'][epoch-1] and val_loss < 1.05*best_val_loss:\n",
        "                best_val_loss = only_save_best_models(best_val_losses, epoch, val_loss, best_val_loss, fold_folder, init_model, optimizer=init_optimizer)\n",
        "        init_model.train()\n",
        "\n",
        "    for k in keys:\n",
        "        if k.startswith('init_'): del globals()[k]\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "\n",
        "    # WHEN ACTUAL TRAINING CAN WATCH THEN SET NUM EPOCHS AFTER FIRST FOLD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8gagFmbKUIpr"
      },
      "outputs": [],
      "source": [
        "# Plot the training/evaluation metrics for each fold\n",
        "for fold in range(ModelParams.n_folds):\n",
        "    num_epochs = len(graph_train_val[fold]['train_loss'])\n",
        "    metrics = ['train_loss', 'val_loss', 'train_f1', 'val_f1']\n",
        "    for metric in metrics:\n",
        "        plt.plot(range(1, num_epochs + 1), graph_train_val[fold][metric])\n",
        "    plt.title('Fold ' + str(fold))\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylim([0, 1])\n",
        "    plt.legend(metrics)\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example Prediction Exploration Cell (to see where your model went wrong - optional)"
      ],
      "metadata": {
        "id": "hVJUPQMZuTFY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_model_predict_cell = False\n",
        "\n",
        "if run_model_predict_cell:\n",
        "    init_model = TokenModel('roberta-large', f\"{pt_folder}/best_epoch/model.pth\")\n",
        "    init_model.load_state_dict(torch.load(f\"{init_folder}/fold_0/epoch_12/model.pth\"))\n",
        "    init_model.to(device)\n",
        "    init_model.eval()\n",
        "\n",
        "    temp = train_dataset.merge(train_df, how='inner', on=['case_num', 'pn_num', 'feature_num'])\n",
        "    temp.reset_index(inplace=True, drop=True)\n",
        "\n",
        "    # 2nd index, pn_num 46 case and feature 0\n",
        "    index = 2\n",
        "    print(temp['pn_history'][index])\n",
        "    tokenized_text = roberta_tokenizer(temp['pn_history'][index],\n",
        "                                    temp['feature_text'][index],\n",
        "                                        add_special_tokens=True,\n",
        "                                        max_length=ModelParams.max_length,\n",
        "                                        padding='max_length',\n",
        "                                        return_offsets_mapping=True)\n",
        "\n",
        "    init_dataset = PatientNotesDataset(train_dataset, train_df, roberta_tokenizer)\n",
        "    init_dataloader = DataLoader(dataset=init_dataset, batch_size=3, num_workers=ModelParams.num_workers, pin_memory=ModelParams.pin_mem)\n",
        "    for batch in init_dataloader:\n",
        "        break\n",
        "    torch.set_printoptions(threshold=10_000_000)\n",
        "    # print(batch['input_ids'])\n",
        "    # print(batch['attention_mask'])\n",
        "    print(batch['labels'])\n",
        "    print(torch.nonzero(batch['labels']))\n",
        "    print(batch['labels'].shape)\n",
        "\n",
        "    batch = {k: v.to(device, non_blocking=True) for k, v in batch.items()}\n",
        "    labels = batch['labels']\n",
        "    del batch['labels']\n",
        "\n",
        "    outputs = init_model(batch)\n",
        "    print(outputs)\n",
        "    print(outputs.shape)\n",
        "\n",
        "    softmax = nn.Softmax(dim=1)\n",
        "    temp = softmax(outputs)[:, 1, :].to('cpu').numpy()\n",
        "    print(temp)\n",
        "    print(torch.mean(temp))\n",
        "\n",
        "    # predictions = torch.argmax(outputs, dim=1)\n",
        "    # f1_metrics = calculate_f1_metrics(predictions=predictions, references=labels, attention_mask=batch['attention_mask'])\n",
        "    # train_true_positive += int(f1_metrics[0])\n",
        "    # train_false_positive += int(f1_metrics[1])\n",
        "    # train_false_negative += int(f1_metrics[2])\n",
        "\n",
        "    loss_fn = nn.CrossEntropyLoss(ce_weights, reduction='mean')\n",
        "    loss_fn(outputs, labels)\n",
        "\n",
        "    labels_2 = torch.zeros(3, 8, dtype=torch.int64)\n",
        "    outputs_2 = torch.zeros(3, 2, 8, dtype=torch.float32)\n",
        "\n",
        "    labels_2[0, :8] = labels[0][178:186]\n",
        "    labels_2[2, :6] = labels[2][206:212]\n",
        "\n",
        "    outputs_2[0, :, 0:8] = outputs[0, :, 178:186]\n",
        "    outputs_2[2, :, 0:6] = outputs[2, :, 206:212]\n",
        "\n",
        "    print(loss_fn(outputs_2, labels_2))\n",
        "    print(outputs_2)\n",
        "    print(labels_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "elFVESihVYgi",
        "outputId": "03b872d5-7e61-47bf-970e-5d5fcd1b76ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mr. cleveland is a 17yo m who was consented by parents to be alone today and provide information.  he has no notable pmh and presented today with palpitations that have been occurring sporadically for 2-3 months.  he started college 7-8 months ago and described feelings of nervousness and anxiousness with starting school and exams.  he endorses using his roommates prescription adderall to study for tests and this coincides with the palpitations at times.  nothing makes his palpitations worse or better when they occur.  ros is negative except for light headedness and sob.  he has not had any cold/heat intolerance, diarrhea, changes in voiding habits, or weight loss.  worried about being able to play basketball.\r\n",
            "\r\n",
            "meds:  adderall from his roommate\r\n",
            "pmh:  none\r\n",
            "psh:  none\r\n",
            "all:  nkda\r\n",
            "family:  mom: thyroid issues, father: heart attack\r\n",
            "social:  no tob, occ etoh, no illicits\r\n",
            "sex:  with gf, use condoms\r\n",
            "diet:  good\r\n",
            "exercise:  basketball\n",
            "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])\n",
            "tensor([[  0, 179],\n",
            "        [  0, 180],\n",
            "        [  0, 181],\n",
            "        [  0, 182],\n",
            "        [  0, 183],\n",
            "        [  0, 184],\n",
            "        [  0, 185],\n",
            "        [  2, 207],\n",
            "        [  2, 208],\n",
            "        [  2, 209],\n",
            "        [  2, 210]])\n",
            "torch.Size([3, 460])\n",
            "tensor([[[-1.1062, -1.0325, -1.0527, -1.1361, -1.1138, -1.0771, -1.0716,\n",
            "          -1.1309, -1.0923, -1.1187, -1.0919, -1.0621, -1.0729, -1.1273,\n",
            "          -1.0957, -1.0580, -1.1240, -1.0821, -1.0645, -1.0662, -1.0870,\n",
            "          -1.0880, -1.0964, -1.1259, -1.0818, -1.0373, -1.1040, -1.0446,\n",
            "          -1.0727, -1.0706, -1.1100, -1.1186, -1.1228, -1.0885, -1.0973,\n",
            "          -1.0852, -1.0821, -1.0582, -1.1134, -1.1429, -1.1441, -1.0676,\n",
            "          -1.0863, -1.0746, -1.0846, -1.0975, -1.0991, -1.1119, -1.0915,\n",
            "          -1.1277, -1.0965, -1.0773, -1.0743, -1.0774, -1.0973, -1.0792,\n",
            "          -1.0222, -1.0735, -1.0799, -1.1123, -1.0536, -1.1172, -1.0608,\n",
            "          -1.1410, -1.1375, -1.1638, -1.1495, -1.1256, -1.0835, -1.0999,\n",
            "          -1.1032, -1.1281, -1.0879, -1.0686, -1.0829, -1.1051, -1.1196,\n",
            "          -1.0655, -1.0736, -1.0495, -1.0753, -1.1134, -1.1412, -1.1085,\n",
            "          -1.1262, -1.0745, -1.0995, -1.0353, -1.1383, -1.0764, -1.0904,\n",
            "          -1.0973, -1.1460, -1.1379, -1.1242, -1.0885, -1.1079, -1.0947,\n",
            "          -1.0751, -1.0906, -1.0932, -1.1005, -1.1214, -1.1097, -1.1265,\n",
            "          -1.1518, -1.1157, -1.1233, -1.0941, -1.1004, -1.1073, -1.1015,\n",
            "          -1.1319, -1.1168, -1.1005, -1.1018, -1.1031, -1.1427, -1.1424,\n",
            "          -1.1233, -1.1278, -1.1384, -1.0797, -1.1076, -1.1267, -1.1323,\n",
            "          -1.1087, -1.1296, -1.0942, -1.1181, -1.1126, -1.1262, -1.1406,\n",
            "          -1.1861, -1.1224, -1.1132, -1.1055, -1.1234, -1.1279, -1.0971,\n",
            "          -1.1159, -1.1069, -1.1227, -1.1249, -1.1391, -1.1508, -1.1133,\n",
            "          -1.1068, -1.0744, -1.1354, -1.0839, -1.0684, -1.0714, -1.0688,\n",
            "          -1.1157, -1.1368, -1.1335, -1.1132, -1.0941, -1.0457, -1.1219,\n",
            "          -1.1102, -1.1017, -1.1165, -1.0836, -1.0955, -1.1143, -1.0919,\n",
            "          -1.1325, -1.0676, -1.1224, -1.1436, -1.0872, -1.1222, -1.1295,\n",
            "          -1.0664, -1.0846, -1.1036, -1.0629, -1.0866, -1.0998, -1.0766,\n",
            "          -1.1204, -1.0667, -1.0997, -1.0788, -1.1209, -1.0999, -1.0722,\n",
            "          -1.1521, -1.1038, -1.1140, -1.0833, -1.0911, -1.0786, -1.0348,\n",
            "          -1.1401, -1.0713, -1.0935, -1.1201, -1.0829, -1.0837, -1.0746,\n",
            "          -1.1110, -1.1462, -1.1101, -1.1306, -1.1579, -1.0729, -1.0892,\n",
            "          -1.0517, -1.0867, -1.1404, -1.0913, -1.1318, -1.0838, -1.1024,\n",
            "          -1.1239, -1.1483, -1.1345, -1.0999, -1.1077, -1.0714, -1.1125,\n",
            "          -1.1206, -1.1230, -1.1019, -1.0875, -1.0611, -1.0834, -1.0821,\n",
            "          -1.1111, -1.1552, -1.1024, -1.1263, -1.1213, -1.1390, -1.1019,\n",
            "          -1.1137, -1.1237, -1.1332, -1.1436, -1.1013, -1.1328, -1.1583,\n",
            "          -1.1330, -1.1286, -1.1166, -1.1679, -1.1383, -1.1331, -1.1407,\n",
            "          -1.1420, -1.1546, -1.1678, -1.1315, -1.1372, -1.1762, -1.0802,\n",
            "          -1.1009, -1.1009, -1.1009, -1.1009, -1.1009, -1.1009, -1.1009,\n",
            "          -1.1009, -1.1009, -1.1009, -1.1009, -1.1009, -1.1009, -1.1009,\n",
            "          -1.1009, -1.1009, -1.1009, -1.1009, -1.1009, -1.1009, -1.1009,\n",
            "          -1.1009, -1.1009, -1.1009, -1.1009, -1.1009, -1.1009, -1.1009,\n",
            "          -1.1009, -1.1009, -1.1009, -1.1009, -1.1009, -1.1009, -1.1009,\n",
            "          -1.1009, -1.1009, -1.1009, -1.1009, -1.1009, -1.1009, -1.1009,\n",
            "          -1.1009, -1.1009, -1.1009, -1.1009, -1.1009, -1.1009, -1.1009,\n",
            "          -1.1009, -1.1009, -1.1009, -1.1009, -1.1009, -1.1009, -1.1009,\n",
            "          -1.1009, -1.1009, -1.1009, -1.1009, -1.1009, -1.1009, -1.1009,\n",
            "          -1.1009, -1.1009, -1.1009, -1.1009, -1.1009, -1.1009, -1.1009,\n",
            "          -1.1009, -1.1009, -1.1009, -1.1009, -1.1009, -1.1009, -1.1009,\n",
            "          -1.1009, -1.1009, -1.1009, -1.1009, -1.1009, -1.1009, -1.1009,\n",
            "          -1.1009, -1.1009, -1.1009, -1.1009, -1.1009, -1.1009, -1.1009,\n",
            "          -1.1009, -1.1009, -1.1009, -1.1009, -1.1009, -1.1009, -1.1009,\n",
            "          -1.1009, -1.1009, -1.1009, -1.1009, -1.1009, -1.1009, -1.1009,\n",
            "          -1.1009, -1.1009, -1.1009, -1.1009, -1.1009, -1.1009, -1.1009,\n",
            "          -1.1009, -1.1009, -1.1009, -1.1009, -1.1009, -1.1009, -1.1009,\n",
            "          -1.1009, -1.1009, -1.1009, -1.1009, -1.1009, -1.1009, -1.1009,\n",
            "          -1.1009, -1.1009, -1.1009, -1.1009, -1.1009, -1.1009, -1.1009,\n",
            "          -1.1009, -1.1009, -1.1009, -1.1009, -1.1009, -1.1009, -1.1009,\n",
            "          -1.1009, -1.1009, -1.1009, -1.1009, -1.1009, -1.1009, -1.1009,\n",
            "          -1.1009, -1.1009, -1.1009, -1.1009, -1.1009, -1.1009, -1.1009,\n",
            "          -1.1009, -1.1009, -1.1009, -1.1009, -1.1009, -1.1009, -1.1009,\n",
            "          -1.1009, -1.1009, -1.1009, -1.1009, -1.1009, -1.1009, -1.1009,\n",
            "          -1.1009, -1.1009, -1.1009, -1.1009, -1.1009, -1.1009, -1.1009,\n",
            "          -1.1009, -1.1009, -1.1009, -1.1009, -1.1009, -1.1009, -1.1009,\n",
            "          -1.1009, -1.1009, -1.1009, -1.1009, -1.1009, -1.1009, -1.1009,\n",
            "          -1.1009, -1.1009, -1.1009, -1.1009, -1.1009, -1.1009, -1.1009,\n",
            "          -1.1009, -1.1009, -1.1009, -1.1009, -1.1009],\n",
            "         [ 0.2648,  0.2848,  0.2901,  0.2067,  0.2388,  0.2759,  0.2829,\n",
            "           0.2584,  0.2723,  0.2526,  0.2740,  0.2949,  0.2682,  0.2127,\n",
            "           0.2529,  0.2819,  0.2500,  0.2756,  0.3177,  0.3239,  0.2842,\n",
            "           0.3024,  0.2611,  0.2205,  0.2596,  0.3299,  0.2330,  0.2883,\n",
            "           0.2835,  0.3006,  0.2374,  0.2928,  0.2599,  0.2233,  0.2370,\n",
            "           0.2461,  0.2715,  0.2747,  0.2385,  0.2060,  0.2027,  0.2616,\n",
            "           0.2596,  0.2850,  0.2559,  0.2095,  0.2217,  0.2290,  0.2709,\n",
            "           0.2477,  0.2608,  0.2124,  0.2666,  0.2573,  0.2324,  0.2507,\n",
            "           0.2982,  0.2918,  0.2414,  0.2267,  0.3005,  0.1983,  0.2533,\n",
            "           0.2238,  0.2133,  0.1743,  0.2105,  0.2246,  0.2461,  0.2311,\n",
            "           0.2218,  0.2155,  0.2278,  0.2630,  0.2721,  0.2151,  0.2331,\n",
            "           0.2693,  0.2555,  0.2832,  0.2901,  0.2495,  0.1912,  0.2319,\n",
            "           0.2477,  0.2702,  0.2854,  0.2878,  0.2369,  0.2300,  0.2538,\n",
            "           0.2451,  0.2015,  0.2055,  0.2069,  0.2776,  0.2196,  0.2298,\n",
            "           0.2647,  0.2457,  0.2733,  0.2404,  0.2477,  0.2524,  0.2328,\n",
            "           0.2277,  0.2169,  0.2291,  0.2541,  0.2191,  0.2475,  0.2450,\n",
            "           0.2436,  0.2361,  0.2585,  0.2590,  0.2289,  0.2164,  0.1931,\n",
            "           0.2265,  0.2222,  0.2147,  0.2626,  0.2679,  0.2444,  0.2382,\n",
            "           0.2399,  0.2513,  0.2516,  0.2556,  0.2420,  0.2617,  0.2202,\n",
            "           0.1884,  0.2405,  0.2315,  0.2428,  0.2129,  0.2374,  0.2594,\n",
            "           0.2440,  0.2417,  0.2520,  0.2084,  0.2213,  0.1738,  0.2178,\n",
            "           0.2355,  0.2685,  0.2121,  0.2863,  0.2789,  0.2817,  0.2689,\n",
            "           0.2349,  0.2328,  0.2528,  0.2426,  0.2511,  0.2881,  0.2178,\n",
            "           0.2613,  0.2265,  0.2094,  0.2275,  0.2542,  0.2268,  0.2503,\n",
            "           0.2234,  0.2886,  0.2302,  0.2046,  0.2819,  0.2133,  0.2168,\n",
            "           0.2675,  0.2461,  0.2629,  0.2460,  0.2606,  0.2488,  0.2721,\n",
            "           0.2577,  0.2980,  0.2697,  0.3008,  0.2188,  0.2459,  0.2690,\n",
            "           0.2135,  0.2734,  0.2303,  0.2435,  0.2666,  0.2706,  0.2980,\n",
            "           0.2174,  0.2384,  0.2783,  0.2550,  0.2453,  0.2325,  0.2701,\n",
            "           0.2448,  0.2007,  0.2503,  0.2349,  0.2045,  0.2679,  0.2349,\n",
            "           0.2783,  0.2445,  0.2167,  0.2499,  0.2241,  0.2443,  0.2366,\n",
            "           0.2147,  0.2235,  0.2129,  0.2454,  0.2810,  0.2513,  0.2297,\n",
            "           0.2358,  0.2405,  0.2292,  0.2424,  0.2817,  0.3051,  0.2312,\n",
            "           0.2083,  0.2368,  0.2622,  0.2232,  0.2434,  0.2378,  0.2393,\n",
            "           0.2610,  0.2308,  0.2400,  0.1970,  0.2419,  0.2158,  0.1874,\n",
            "           0.2090,  0.2280,  0.2576,  0.1399,  0.1867,  0.2228,  0.2234,\n",
            "           0.2223,  0.1916,  0.2011,  0.1838,  0.2319,  0.1799,  0.2624,\n",
            "           0.1389,  0.1389,  0.1389,  0.1389,  0.1389,  0.1389,  0.1389,\n",
            "           0.1389,  0.1389,  0.1389,  0.1389,  0.1389,  0.1389,  0.1389,\n",
            "           0.1389,  0.1389,  0.1389,  0.1389,  0.1389,  0.1389,  0.1389,\n",
            "           0.1389,  0.1389,  0.1389,  0.1389,  0.1389,  0.1389,  0.1389,\n",
            "           0.1389,  0.1389,  0.1389,  0.1389,  0.1389,  0.1389,  0.1389,\n",
            "           0.1389,  0.1389,  0.1389,  0.1389,  0.1389,  0.1389,  0.1389,\n",
            "           0.1389,  0.1389,  0.1389,  0.1389,  0.1389,  0.1389,  0.1389,\n",
            "           0.1389,  0.1389,  0.1389,  0.1389,  0.1389,  0.1389,  0.1389,\n",
            "           0.1389,  0.1389,  0.1389,  0.1389,  0.1389,  0.1389,  0.1389,\n",
            "           0.1389,  0.1389,  0.1389,  0.1389,  0.1389,  0.1389,  0.1389,\n",
            "           0.1389,  0.1389,  0.1389,  0.1389,  0.1389,  0.1389,  0.1389,\n",
            "           0.1389,  0.1389,  0.1389,  0.1389,  0.1389,  0.1389,  0.1389,\n",
            "           0.1389,  0.1389,  0.1389,  0.1389,  0.1389,  0.1389,  0.1389,\n",
            "           0.1389,  0.1389,  0.1389,  0.1389,  0.1389,  0.1389,  0.1389,\n",
            "           0.1389,  0.1389,  0.1389,  0.1389,  0.1389,  0.1389,  0.1389,\n",
            "           0.1389,  0.1389,  0.1389,  0.1389,  0.1389,  0.1389,  0.1389,\n",
            "           0.1389,  0.1389,  0.1389,  0.1389,  0.1389,  0.1389,  0.1389,\n",
            "           0.1389,  0.1389,  0.1389,  0.1389,  0.1389,  0.1389,  0.1389,\n",
            "           0.1389,  0.1389,  0.1389,  0.1389,  0.1389,  0.1389,  0.1389,\n",
            "           0.1389,  0.1389,  0.1389,  0.1389,  0.1389,  0.1389,  0.1389,\n",
            "           0.1389,  0.1389,  0.1389,  0.1389,  0.1389,  0.1389,  0.1389,\n",
            "           0.1389,  0.1389,  0.1389,  0.1389,  0.1389,  0.1389,  0.1389,\n",
            "           0.1389,  0.1389,  0.1389,  0.1389,  0.1389,  0.1389,  0.1389,\n",
            "           0.1389,  0.1389,  0.1389,  0.1389,  0.1389,  0.1389,  0.1389,\n",
            "           0.1389,  0.1389,  0.1389,  0.1389,  0.1389,  0.1389,  0.1389,\n",
            "           0.1389,  0.1389,  0.1389,  0.1389,  0.1389,  0.1389,  0.1389,\n",
            "           0.1389,  0.1389,  0.1389,  0.1389,  0.1389,  0.1389,  0.1389,\n",
            "           0.1389,  0.1389,  0.1389,  0.1389,  0.1389,  0.1389,  0.1389,\n",
            "           0.1389,  0.1389,  0.1389,  0.1389,  0.1389]],\n",
            "\n",
            "        [[-1.1165, -1.1316, -1.0902, -1.1271, -1.0759, -1.0632, -1.0658,\n",
            "          -1.0901, -1.0866, -1.0972, -1.1139, -1.1166, -1.0646, -1.1191,\n",
            "          -1.0638, -1.0892, -1.0714, -1.1090, -1.1353, -1.0987, -1.0648,\n",
            "          -1.0967, -1.1599, -1.1031, -1.0818, -1.1091, -1.1374, -1.1135,\n",
            "          -1.1667, -1.1626, -1.1084, -1.0769, -1.1225, -1.0901, -1.0838,\n",
            "          -1.0920, -1.0651, -1.0634, -1.1013, -1.0888, -1.0610, -1.1129,\n",
            "          -1.1441, -1.0951, -1.0973, -1.0736, -1.0877, -1.0772, -1.0848,\n",
            "          -1.1070, -1.1009, -1.0862, -1.1294, -1.0991, -1.0905, -1.1325,\n",
            "          -1.0616, -1.0914, -1.1456, -1.0978, -1.0946, -1.0907, -1.0854,\n",
            "          -1.1135, -1.1148, -1.1032, -1.0831, -1.0870, -1.1088, -1.1131,\n",
            "          -1.1281, -1.1617, -1.1435, -1.1388, -1.1207, -1.1252, -1.1141,\n",
            "          -1.0613, -1.0847, -1.1176, -1.1443, -1.1170, -1.1683, -1.1603,\n",
            "          -1.1221, -1.0900, -1.1096, -1.1265, -1.0886, -1.1128, -1.1266,\n",
            "          -1.0831, -1.0850, -1.1280, -1.1235, -1.0763, -1.1059, -1.1419,\n",
            "          -1.1240, -1.0940, -1.1003, -1.0727, -1.0663, -1.0490, -1.0747,\n",
            "          -1.0656, -1.0041, -1.1009, -1.0948, -1.0766, -1.0480, -1.0310,\n",
            "          -1.0775, -1.1047, -1.0885, -1.0892, -1.0942, -1.1416, -1.1297,\n",
            "          -1.1082, -1.1229, -1.1337, -1.1222, -1.1253, -1.1008, -1.1346,\n",
            "          -1.1235, -1.1110, -1.0911, -1.0794, -1.0716, -1.0663, -1.0898,\n",
            "          -1.0989, -1.0888, -1.0711, -1.0300, -1.0710, -1.0851, -1.1007,\n",
            "          -1.1486, -1.1151, -1.0810, -1.1075, -1.0890, -1.0996, -1.0983,\n",
            "          -1.1233, -1.0875, -1.1353, -1.1420, -1.1260, -1.1450, -1.0930,\n",
            "          -1.0974, -1.1015, -1.0830, -1.1287, -1.0853, -1.1322, -1.1410,\n",
            "          -1.1189, -1.1444, -1.1519, -1.1385, -1.1145, -1.1189, -1.1148,\n",
            "          -1.1248, -1.1272, -1.1044, -1.1139, -1.0848, -1.0836, -1.0630,\n",
            "          -1.0831, -1.0897, -1.1309, -1.0984, -1.1034, -1.1210, -1.1181,\n",
            "          -1.0657, -1.1095, -1.0854, -1.0924, -1.1218, -1.1700, -1.1280,\n",
            "          -1.1435, -1.1467, -1.1432, -1.1576, -1.1312, -1.1436, -1.1635,\n",
            "          -1.1791, -1.1753, -1.1445, -1.1296, -1.1806, -1.0853, -1.0939,\n",
            "          -1.0939, -1.0939, -1.0939, -1.0939, -1.0939, -1.0939, -1.0939,\n",
            "          -1.0939, -1.0939, -1.0939, -1.0939, -1.0939, -1.0939, -1.0939,\n",
            "          -1.0939, -1.0939, -1.0939, -1.0939, -1.0939, -1.0939, -1.0939,\n",
            "          -1.0939, -1.0939, -1.0939, -1.0939, -1.0939, -1.0939, -1.0939,\n",
            "          -1.0939, -1.0939, -1.0939, -1.0939, -1.0939, -1.0939, -1.0939,\n",
            "          -1.0939, -1.0939, -1.0939, -1.0939, -1.0939, -1.0939, -1.0939,\n",
            "          -1.0939, -1.0939, -1.0939, -1.0939, -1.0939, -1.0939, -1.0939,\n",
            "          -1.0939, -1.0939, -1.0939, -1.0939, -1.0939, -1.0939, -1.0939,\n",
            "          -1.0939, -1.0939, -1.0939, -1.0939, -1.0939, -1.0939, -1.0939,\n",
            "          -1.0939, -1.0939, -1.0939, -1.0939, -1.0939, -1.0939, -1.0939,\n",
            "          -1.0939, -1.0939, -1.0939, -1.0939, -1.0939, -1.0939, -1.0939,\n",
            "          -1.0939, -1.0939, -1.0939, -1.0939, -1.0939, -1.0939, -1.0939,\n",
            "          -1.0939, -1.0939, -1.0939, -1.0939, -1.0939, -1.0939, -1.0939,\n",
            "          -1.0939, -1.0939, -1.0939, -1.0939, -1.0939, -1.0939, -1.0939,\n",
            "          -1.0939, -1.0939, -1.0939, -1.0939, -1.0939, -1.0939, -1.0939,\n",
            "          -1.0939, -1.0939, -1.0939, -1.0939, -1.0939, -1.0939, -1.0939,\n",
            "          -1.0939, -1.0939, -1.0939, -1.0939, -1.0939, -1.0939, -1.0939,\n",
            "          -1.0939, -1.0939, -1.0939, -1.0939, -1.0939, -1.0939, -1.0939,\n",
            "          -1.0939, -1.0939, -1.0939, -1.0939, -1.0939, -1.0939, -1.0939,\n",
            "          -1.0939, -1.0939, -1.0939, -1.0939, -1.0939, -1.0939, -1.0939,\n",
            "          -1.0939, -1.0939, -1.0939, -1.0939, -1.0939, -1.0939, -1.0939,\n",
            "          -1.0939, -1.0939, -1.0939, -1.0939, -1.0939, -1.0939, -1.0939,\n",
            "          -1.0939, -1.0939, -1.0939, -1.0939, -1.0939, -1.0939, -1.0939,\n",
            "          -1.0939, -1.0939, -1.0939, -1.0939, -1.0939, -1.0939, -1.0939,\n",
            "          -1.0939, -1.0939, -1.0939, -1.0939, -1.0939, -1.0939, -1.0939,\n",
            "          -1.0939, -1.0939, -1.0939, -1.0939, -1.0939, -1.0939, -1.0939,\n",
            "          -1.0939, -1.0939, -1.0939, -1.0939, -1.0939, -1.0939, -1.0939,\n",
            "          -1.0939, -1.0939, -1.0939, -1.0939, -1.0939, -1.0939, -1.0939,\n",
            "          -1.0939, -1.0939, -1.0939, -1.0939, -1.0939, -1.0939, -1.0939,\n",
            "          -1.0939, -1.0939, -1.0939, -1.0939, -1.0939, -1.0939, -1.0939,\n",
            "          -1.0939, -1.0939, -1.0939, -1.0939, -1.0939, -1.0939, -1.0939,\n",
            "          -1.0939, -1.0939, -1.0939, -1.0939, -1.0939, -1.0939, -1.0939,\n",
            "          -1.0939, -1.0939, -1.0939, -1.0939, -1.0939, -1.0939, -1.0939,\n",
            "          -1.0939, -1.0939, -1.0939, -1.0939, -1.0939, -1.0939, -1.0939,\n",
            "          -1.0939, -1.0939, -1.0939, -1.0939, -1.0939, -1.0939, -1.0939,\n",
            "          -1.0939, -1.0939, -1.0939, -1.0939, -1.0939, -1.0939, -1.0939,\n",
            "          -1.0939, -1.0939, -1.0939, -1.0939, -1.0939],\n",
            "         [ 0.2346,  0.2442,  0.2877,  0.2267,  0.2845,  0.2922,  0.2617,\n",
            "           0.2615,  0.2787,  0.2579,  0.2483,  0.2331,  0.2912,  0.2412,\n",
            "           0.2542,  0.2453,  0.3144,  0.2396,  0.2338,  0.2511,  0.2922,\n",
            "           0.2671,  0.1915,  0.2702,  0.3265,  0.2555,  0.1987,  0.2157,\n",
            "           0.2037,  0.1462,  0.2224,  0.2573,  0.2445,  0.2460,  0.2471,\n",
            "           0.2214,  0.2888,  0.3070,  0.2465,  0.2578,  0.2887,  0.2286,\n",
            "           0.2284,  0.2535,  0.2596,  0.2769,  0.2530,  0.3148,  0.2537,\n",
            "           0.2574,  0.2484,  0.2515,  0.2354,  0.2399,  0.2508,  0.2134,\n",
            "           0.2671,  0.2438,  0.2073,  0.2473,  0.2591,  0.2675,  0.2513,\n",
            "           0.2190,  0.2333,  0.2553,  0.2502,  0.2465,  0.2343,  0.2346,\n",
            "           0.2209,  0.1864,  0.2201,  0.2131,  0.1801,  0.2420,  0.2227,\n",
            "           0.2758,  0.2489,  0.2109,  0.2104,  0.2281,  0.1806,  0.1760,\n",
            "           0.2348,  0.2566,  0.2453,  0.2301,  0.2846,  0.2162,  0.2060,\n",
            "           0.2594,  0.2814,  0.2102,  0.2373,  0.2362,  0.2470,  0.2178,\n",
            "           0.2194,  0.2639,  0.2601,  0.2812,  0.3060,  0.3037,  0.2787,\n",
            "           0.3030,  0.3473,  0.2417,  0.2482,  0.2795,  0.3024,  0.3089,\n",
            "           0.2808,  0.2484,  0.2809,  0.2641,  0.2707,  0.2418,  0.2580,\n",
            "           0.2459,  0.2224,  0.1896,  0.2468,  0.2469,  0.2746,  0.2121,\n",
            "           0.1971,  0.2054,  0.2600,  0.2750,  0.2881,  0.3169,  0.2458,\n",
            "           0.2640,  0.2772,  0.2841,  0.2949,  0.2695,  0.2197,  0.2522,\n",
            "           0.2392,  0.2346,  0.2612,  0.2537,  0.2509,  0.2323,  0.2508,\n",
            "           0.2505,  0.2870,  0.2611,  0.2263,  0.2695,  0.2571,  0.2403,\n",
            "           0.2595,  0.2647,  0.2991,  0.2284,  0.2710,  0.2014,  0.2514,\n",
            "           0.2266,  0.2107,  0.1720,  0.2395,  0.2204,  0.2267,  0.1969,\n",
            "           0.2239,  0.2214,  0.2606,  0.2445,  0.2861,  0.2962,  0.3082,\n",
            "           0.2980,  0.2572,  0.2435,  0.2806,  0.2348,  0.2191,  0.2387,\n",
            "           0.3083,  0.2596,  0.2529,  0.2693,  0.2126,  0.1887,  0.2021,\n",
            "           0.2119,  0.2325,  0.1852,  0.1790,  0.2035,  0.2054,  0.1934,\n",
            "           0.1777,  0.1784,  0.1709,  0.2076,  0.1681,  0.2366,  0.1314,\n",
            "           0.1314,  0.1314,  0.1314,  0.1314,  0.1314,  0.1314,  0.1314,\n",
            "           0.1314,  0.1314,  0.1314,  0.1314,  0.1314,  0.1314,  0.1314,\n",
            "           0.1314,  0.1314,  0.1314,  0.1314,  0.1314,  0.1314,  0.1314,\n",
            "           0.1314,  0.1314,  0.1314,  0.1314,  0.1314,  0.1314,  0.1314,\n",
            "           0.1314,  0.1314,  0.1314,  0.1314,  0.1314,  0.1314,  0.1314,\n",
            "           0.1314,  0.1314,  0.1314,  0.1314,  0.1314,  0.1314,  0.1314,\n",
            "           0.1314,  0.1314,  0.1314,  0.1314,  0.1314,  0.1314,  0.1314,\n",
            "           0.1314,  0.1314,  0.1314,  0.1314,  0.1314,  0.1314,  0.1314,\n",
            "           0.1314,  0.1314,  0.1314,  0.1314,  0.1314,  0.1314,  0.1314,\n",
            "           0.1314,  0.1314,  0.1314,  0.1314,  0.1314,  0.1314,  0.1314,\n",
            "           0.1314,  0.1314,  0.1314,  0.1314,  0.1314,  0.1314,  0.1314,\n",
            "           0.1314,  0.1314,  0.1314,  0.1314,  0.1314,  0.1314,  0.1314,\n",
            "           0.1314,  0.1314,  0.1314,  0.1314,  0.1314,  0.1314,  0.1314,\n",
            "           0.1314,  0.1314,  0.1314,  0.1314,  0.1314,  0.1314,  0.1314,\n",
            "           0.1314,  0.1314,  0.1314,  0.1314,  0.1314,  0.1314,  0.1314,\n",
            "           0.1314,  0.1314,  0.1314,  0.1314,  0.1314,  0.1314,  0.1314,\n",
            "           0.1314,  0.1314,  0.1314,  0.1314,  0.1314,  0.1314,  0.1314,\n",
            "           0.1314,  0.1314,  0.1314,  0.1314,  0.1314,  0.1314,  0.1314,\n",
            "           0.1314,  0.1314,  0.1314,  0.1314,  0.1314,  0.1314,  0.1314,\n",
            "           0.1314,  0.1314,  0.1314,  0.1314,  0.1314,  0.1314,  0.1314,\n",
            "           0.1314,  0.1314,  0.1314,  0.1314,  0.1314,  0.1314,  0.1314,\n",
            "           0.1314,  0.1314,  0.1314,  0.1314,  0.1314,  0.1314,  0.1314,\n",
            "           0.1314,  0.1314,  0.1314,  0.1314,  0.1314,  0.1314,  0.1314,\n",
            "           0.1314,  0.1314,  0.1314,  0.1314,  0.1314,  0.1314,  0.1314,\n",
            "           0.1314,  0.1314,  0.1314,  0.1314,  0.1314,  0.1314,  0.1314,\n",
            "           0.1314,  0.1314,  0.1314,  0.1314,  0.1314,  0.1314,  0.1314,\n",
            "           0.1314,  0.1314,  0.1314,  0.1314,  0.1314,  0.1314,  0.1314,\n",
            "           0.1314,  0.1314,  0.1314,  0.1314,  0.1314,  0.1314,  0.1314,\n",
            "           0.1314,  0.1314,  0.1314,  0.1314,  0.1314,  0.1314,  0.1314,\n",
            "           0.1314,  0.1314,  0.1314,  0.1314,  0.1314,  0.1314,  0.1314,\n",
            "           0.1314,  0.1314,  0.1314,  0.1314,  0.1314,  0.1314,  0.1314,\n",
            "           0.1314,  0.1314,  0.1314,  0.1314,  0.1314,  0.1314,  0.1314,\n",
            "           0.1314,  0.1314,  0.1314,  0.1314,  0.1314,  0.1314,  0.1314,\n",
            "           0.1314,  0.1314,  0.1314,  0.1314,  0.1314,  0.1314,  0.1314,\n",
            "           0.1314,  0.1314,  0.1314,  0.1314,  0.1314,  0.1314,  0.1314,\n",
            "           0.1314,  0.1314,  0.1314,  0.1314,  0.1314,  0.1314,  0.1314,\n",
            "           0.1314,  0.1314,  0.1314,  0.1314,  0.1314]],\n",
            "\n",
            "        [[-1.1365, -1.0571, -1.1151, -1.1485, -1.1103, -1.1299, -1.1260,\n",
            "          -1.1230, -1.0959, -1.1166, -1.1125, -1.0978, -1.0995, -1.1154,\n",
            "          -1.1159, -1.1300, -1.0716, -1.0766, -1.0501, -1.1245, -1.1029,\n",
            "          -1.1074, -1.0948, -1.2335, -1.1497, -1.1007, -1.1383, -1.1063,\n",
            "          -1.0443, -1.0818, -1.0493, -1.1091, -1.1343, -1.1317, -1.1257,\n",
            "          -1.1171, -1.0998, -1.0746, -1.1133, -1.1042, -1.0340, -1.0840,\n",
            "          -1.0964, -1.0966, -1.1318, -1.1001, -1.1160, -1.1464, -1.0849,\n",
            "          -1.0673, -1.1268, -1.1649, -1.1106, -1.0815, -1.1507, -1.1196,\n",
            "          -1.1555, -1.0726, -1.0916, -1.0974, -1.1169, -1.1213, -1.1245,\n",
            "          -1.1245, -1.1460, -1.1252, -1.1100, -1.0959, -1.1271, -1.1145,\n",
            "          -1.0887, -1.1596, -1.1004, -1.0818, -1.1153, -1.1648, -1.1022,\n",
            "          -1.0814, -1.0753, -1.1328, -1.1030, -1.0758, -1.1218, -1.0950,\n",
            "          -1.1176, -1.1187, -1.0806, -1.0647, -1.1113, -1.1302, -1.1242,\n",
            "          -1.1170, -1.1375, -1.0929, -1.0852, -1.0878, -1.1145, -1.1043,\n",
            "          -1.0737, -1.1174, -1.1194, -1.0986, -1.1673, -1.1172, -1.0864,\n",
            "          -1.1206, -1.1118, -1.0979, -1.0712, -1.1126, -1.0954, -1.1219,\n",
            "          -1.1518, -1.1353, -1.0998, -1.1013, -1.1550, -1.0902, -1.0950,\n",
            "          -1.1245, -1.0574, -1.0856, -1.1576, -1.1448, -1.1158, -1.1069,\n",
            "          -1.1125, -1.0781, -1.1208, -1.1419, -1.1161, -1.1130, -1.1282,\n",
            "          -1.1129, -1.0848, -1.1073, -1.1214, -1.1026, -1.0470, -1.0902,\n",
            "          -1.0962, -1.1019, -1.1269, -1.1346, -1.1600, -1.1183, -1.0742,\n",
            "          -1.1154, -1.1225, -1.1325, -1.1164, -1.1105, -1.1410, -1.1273,\n",
            "          -1.1233, -1.0780, -1.0421, -1.0742, -1.0817, -1.0433, -1.1068,\n",
            "          -1.1120, -1.0882, -1.0977, -1.0799, -1.0617, -1.0544, -1.1248,\n",
            "          -1.1196, -1.1092, -1.1375, -1.0956, -1.1450, -1.1219, -1.0981,\n",
            "          -1.1598, -1.1099, -1.0688, -1.0481, -1.1401, -1.1051, -1.1160,\n",
            "          -1.1153, -1.0995, -1.0765, -1.0430, -1.1413, -1.1113, -1.0877,\n",
            "          -1.1384, -1.1251, -1.0823, -1.1339, -1.1414, -1.1223, -1.0911,\n",
            "          -1.1007, -1.1021, -1.1054, -1.1454, -1.1339, -1.1589, -1.1082,\n",
            "          -1.1564, -1.1290, -1.0917, -1.1582, -1.1289, -1.1875, -1.1641,\n",
            "          -1.1339, -1.1322, -1.1210, -1.1198, -1.1118, -1.1394, -1.1016,\n",
            "          -1.0725, -1.0953, -1.0765, -1.0685, -1.0710, -1.1242, -1.1303,\n",
            "          -1.1344, -1.1666, -1.1515, -1.1235, -1.1101, -1.1427, -1.1326,\n",
            "          -1.1555, -1.1373, -1.1863, -1.1956, -1.1398, -1.1511, -1.1434,\n",
            "          -1.1328, -1.0847, -1.0874, -1.1088, -1.1250, -1.1320, -1.1615,\n",
            "          -1.1498, -1.1174, -1.0888, -1.0659, -1.1407, -1.1363, -1.0561,\n",
            "          -1.1079, -1.1400, -1.1664, -1.1326, -1.1406, -1.1274, -1.1509,\n",
            "          -1.1586, -1.1400, -1.1521, -1.1559, -1.1730, -1.1681, -1.1369,\n",
            "          -1.1124, -1.1746, -1.0980, -1.1070, -1.1070, -1.1070, -1.1070,\n",
            "          -1.1070, -1.1070, -1.1070, -1.1070, -1.1070, -1.1070, -1.1070,\n",
            "          -1.1070, -1.1070, -1.1070, -1.1070, -1.1070, -1.1070, -1.1070,\n",
            "          -1.1070, -1.1070, -1.1070, -1.1070, -1.1070, -1.1070, -1.1070,\n",
            "          -1.1070, -1.1070, -1.1070, -1.1070, -1.1070, -1.1070, -1.1070,\n",
            "          -1.1070, -1.1070, -1.1070, -1.1070, -1.1070, -1.1070, -1.1070,\n",
            "          -1.1070, -1.1070, -1.1070, -1.1070, -1.1070, -1.1070, -1.1070,\n",
            "          -1.1070, -1.1070, -1.1070, -1.1070, -1.1070, -1.1070, -1.1070,\n",
            "          -1.1070, -1.1070, -1.1070, -1.1070, -1.1070, -1.1070, -1.1070,\n",
            "          -1.1070, -1.1070, -1.1070, -1.1070, -1.1070, -1.1070, -1.1070,\n",
            "          -1.1070, -1.1070, -1.1070, -1.1070, -1.1070, -1.1070, -1.1070,\n",
            "          -1.1070, -1.1070, -1.1070, -1.1070, -1.1070, -1.1070, -1.1070,\n",
            "          -1.1070, -1.1070, -1.1070, -1.1070, -1.1070, -1.1070, -1.1070,\n",
            "          -1.1070, -1.1070, -1.1070, -1.1070, -1.1070, -1.1070, -1.1070,\n",
            "          -1.1070, -1.1070, -1.1070, -1.1070, -1.1070, -1.1070, -1.1070,\n",
            "          -1.1070, -1.1070, -1.1070, -1.1070, -1.1070, -1.1070, -1.1070,\n",
            "          -1.1070, -1.1070, -1.1070, -1.1070, -1.1070, -1.1070, -1.1070,\n",
            "          -1.1070, -1.1070, -1.1070, -1.1070, -1.1070, -1.1070, -1.1070,\n",
            "          -1.1070, -1.1070, -1.1070, -1.1070, -1.1070, -1.1070, -1.1070,\n",
            "          -1.1070, -1.1070, -1.1070, -1.1070, -1.1070, -1.1070, -1.1070,\n",
            "          -1.1070, -1.1070, -1.1070, -1.1070, -1.1070, -1.1070, -1.1070,\n",
            "          -1.1070, -1.1070, -1.1070, -1.1070, -1.1070, -1.1070, -1.1070,\n",
            "          -1.1070, -1.1070, -1.1070, -1.1070, -1.1070, -1.1070, -1.1070,\n",
            "          -1.1070, -1.1070, -1.1070, -1.1070, -1.1070, -1.1070, -1.1070,\n",
            "          -1.1070, -1.1070, -1.1070, -1.1070, -1.1070, -1.1070, -1.1070,\n",
            "          -1.1070, -1.1070, -1.1070, -1.1070, -1.1070, -1.1070, -1.1070,\n",
            "          -1.1070, -1.1070, -1.1070, -1.1070, -1.1070, -1.1070, -1.1070,\n",
            "          -1.1070, -1.1070, -1.1070, -1.1070, -1.1070],\n",
            "         [ 0.2402,  0.2692,  0.2310,  0.2222,  0.2095,  0.2113,  0.2449,\n",
            "           0.2497,  0.2882,  0.2589,  0.2203,  0.2506,  0.2355,  0.2480,\n",
            "           0.2426,  0.2406,  0.2760,  0.2832,  0.2353,  0.2561,  0.2272,\n",
            "           0.2351,  0.2641,  0.2249,  0.2040,  0.2339,  0.2196,  0.2486,\n",
            "           0.2732,  0.2797,  0.2976,  0.2053,  0.2274,  0.2430,  0.2438,\n",
            "           0.2472,  0.2564,  0.2825,  0.2397,  0.2187,  0.3273,  0.2812,\n",
            "           0.2827,  0.3150,  0.2537,  0.2284,  0.2371,  0.2451,  0.2548,\n",
            "           0.3041,  0.2317,  0.1984,  0.2263,  0.3012,  0.2199,  0.2516,\n",
            "           0.2389,  0.2615,  0.2967,  0.2673,  0.2077,  0.1767,  0.2572,\n",
            "           0.2452,  0.1910,  0.2283,  0.2339,  0.2486,  0.2440,  0.2564,\n",
            "           0.2837,  0.2043,  0.2547,  0.2769,  0.2325,  0.1982,  0.2347,\n",
            "           0.2282,  0.2788,  0.2348,  0.2414,  0.2572,  0.2732,  0.2547,\n",
            "           0.2080,  0.2529,  0.2701,  0.2882,  0.2679,  0.2245,  0.2512,\n",
            "           0.2082,  0.2051,  0.2553,  0.2620,  0.2699,  0.2457,  0.2555,\n",
            "           0.2794,  0.2698,  0.2744,  0.2559,  0.1982,  0.2300,  0.2407,\n",
            "           0.2523,  0.2471,  0.2591,  0.2809,  0.2572,  0.2424,  0.2605,\n",
            "           0.2277,  0.2316,  0.2337,  0.2381,  0.2020,  0.2466,  0.2754,\n",
            "           0.2504,  0.2936,  0.2645,  0.2040,  0.2529,  0.2751,  0.2804,\n",
            "           0.2459,  0.2734,  0.2421,  0.2253,  0.2079,  0.2292,  0.2297,\n",
            "           0.2564,  0.2506,  0.2531,  0.2454,  0.2574,  0.2640,  0.2598,\n",
            "           0.2417,  0.2588,  0.2526,  0.2289,  0.1963,  0.2354,  0.2729,\n",
            "           0.2462,  0.2377,  0.2429,  0.2734,  0.2455,  0.2161,  0.2190,\n",
            "           0.2131,  0.2323,  0.2876,  0.2587,  0.2984,  0.3064,  0.2466,\n",
            "           0.2440,  0.2608,  0.2690,  0.2935,  0.3349,  0.3110,  0.2315,\n",
            "           0.2531,  0.1900,  0.2191,  0.2451,  0.2206,  0.2036,  0.2162,\n",
            "           0.2197,  0.2574,  0.2797,  0.2965,  0.2381,  0.2749,  0.2513,\n",
            "           0.2352,  0.2612,  0.2881,  0.3111,  0.2376,  0.2617,  0.2735,\n",
            "           0.2204,  0.2483,  0.2902,  0.2486,  0.2487,  0.2504,  0.2798,\n",
            "           0.2948,  0.2566,  0.2391,  0.2205,  0.2394,  0.2393,  0.2713,\n",
            "           0.2355,  0.2615,  0.2931,  0.2206,  0.2301,  0.2015,  0.2321,\n",
            "           0.2692,  0.2021,  0.2444,  0.2648,  0.2501,  0.2497,  0.2482,\n",
            "           0.2743,  0.2471,  0.2773,  0.2961,  0.2980,  0.2187,  0.2182,\n",
            "           0.2090,  0.2011,  0.2206,  0.2149,  0.2161,  0.2330,  0.2241,\n",
            "           0.2063,  0.2082,  0.1988,  0.1903,  0.2238,  0.2043,  0.2104,\n",
            "           0.2068,  0.2356,  0.2747,  0.2474,  0.2212,  0.2291,  0.2248,\n",
            "           0.2021,  0.2499,  0.2754,  0.2977,  0.2408,  0.2394,  0.3023,\n",
            "           0.2444,  0.2105,  0.1923,  0.2046,  0.2250,  0.2492,  0.1873,\n",
            "           0.1834,  0.2045,  0.2050,  0.2029,  0.1974,  0.1998,  0.1942,\n",
            "           0.2401,  0.1934,  0.2440,  0.0925,  0.0925,  0.0925,  0.0925,\n",
            "           0.0925,  0.0925,  0.0925,  0.0925,  0.0925,  0.0925,  0.0925,\n",
            "           0.0925,  0.0925,  0.0925,  0.0925,  0.0925,  0.0925,  0.0925,\n",
            "           0.0925,  0.0925,  0.0925,  0.0925,  0.0925,  0.0925,  0.0925,\n",
            "           0.0925,  0.0925,  0.0925,  0.0925,  0.0925,  0.0925,  0.0925,\n",
            "           0.0925,  0.0925,  0.0925,  0.0925,  0.0925,  0.0925,  0.0925,\n",
            "           0.0925,  0.0925,  0.0925,  0.0925,  0.0925,  0.0925,  0.0925,\n",
            "           0.0925,  0.0925,  0.0925,  0.0925,  0.0925,  0.0925,  0.0925,\n",
            "           0.0925,  0.0925,  0.0925,  0.0925,  0.0925,  0.0925,  0.0925,\n",
            "           0.0925,  0.0925,  0.0925,  0.0925,  0.0925,  0.0925,  0.0925,\n",
            "           0.0925,  0.0925,  0.0925,  0.0925,  0.0925,  0.0925,  0.0925,\n",
            "           0.0925,  0.0925,  0.0925,  0.0925,  0.0925,  0.0925,  0.0925,\n",
            "           0.0925,  0.0925,  0.0925,  0.0925,  0.0925,  0.0925,  0.0925,\n",
            "           0.0925,  0.0925,  0.0925,  0.0925,  0.0925,  0.0925,  0.0925,\n",
            "           0.0925,  0.0925,  0.0925,  0.0925,  0.0925,  0.0925,  0.0925,\n",
            "           0.0925,  0.0925,  0.0925,  0.0925,  0.0925,  0.0925,  0.0925,\n",
            "           0.0925,  0.0925,  0.0925,  0.0925,  0.0925,  0.0925,  0.0925,\n",
            "           0.0925,  0.0925,  0.0925,  0.0925,  0.0925,  0.0925,  0.0925,\n",
            "           0.0925,  0.0925,  0.0925,  0.0925,  0.0925,  0.0925,  0.0925,\n",
            "           0.0925,  0.0925,  0.0925,  0.0925,  0.0925,  0.0925,  0.0925,\n",
            "           0.0925,  0.0925,  0.0925,  0.0925,  0.0925,  0.0925,  0.0925,\n",
            "           0.0925,  0.0925,  0.0925,  0.0925,  0.0925,  0.0925,  0.0925,\n",
            "           0.0925,  0.0925,  0.0925,  0.0925,  0.0925,  0.0925,  0.0925,\n",
            "           0.0925,  0.0925,  0.0925,  0.0925,  0.0925,  0.0925,  0.0925,\n",
            "           0.0925,  0.0925,  0.0925,  0.0925,  0.0925,  0.0925,  0.0925,\n",
            "           0.0925,  0.0925,  0.0925,  0.0925,  0.0925,  0.0925,  0.0925,\n",
            "           0.0925,  0.0925,  0.0925,  0.0925,  0.0925,  0.0925,  0.0925,\n",
            "           0.0925,  0.0925,  0.0925,  0.0925,  0.0925]]], device='cuda:0',\n",
            "       grad_fn=<TransposeBackward1>)\n",
            "torch.Size([3, 2, 460])\n",
            "tensor(0.5891, grad_fn=<NllLoss2DBackward0>)\n",
            "tensor([[[-1.0629, -1.0866, -1.0998, -1.0766, -1.1204, -1.0667, -1.0997,\n",
            "          -1.0788],\n",
            "         [ 0.2460,  0.2606,  0.2488,  0.2721,  0.2577,  0.2980,  0.2697,\n",
            "           0.3008]],\n",
            "\n",
            "        [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000]],\n",
            "\n",
            "        [[-1.1582, -1.1289, -1.1875, -1.1641, -1.1339, -1.1322,  0.0000,\n",
            "           0.0000],\n",
            "         [ 0.2206,  0.2301,  0.2015,  0.2321,  0.2692,  0.2021,  0.0000,\n",
            "           0.0000]]], grad_fn=<CopySlices>)\n",
            "tensor([[0, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 1, 1, 1, 1, 0, 0, 0]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzfbV9BEWLce"
      },
      "source": [
        "### Managing Storage\n",
        "\n",
        "After we decide on the best epoch numbers for each fold, keep only the weights for the best epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mR_EUT5tXPaZ"
      },
      "outputs": [],
      "source": [
        "# Manual Input\n",
        "best_epochs = {0: 1,\n",
        "               1: 0,\n",
        "               2: 0,\n",
        "               3: 0,\n",
        "               4: 0}\n",
        "\n",
        "# For all folds keep only the best epoch\n",
        "for fold in range(ModelParams.n_folds):\n",
        "    fold_folder = f\"{init_folder}/fold_{fold}\"\n",
        "    del_epochs = [folder.name for folder in os.scandir(fold_folder) if folder.is_dir()]\n",
        "    del_epochs.remove(f\"epoch_{best_epochs[fold]}\")\n",
        "\n",
        "    for del_epoch in del_epochs: shutil.rmtree(f\"{init_folder}/fold_{fold}/{del_epoch}\")   # remove all directories not containing best epoch\n",
        "\n",
        "    os.renames(f\"{init_folder}/fold_{fold}/epoch_{best_epochs[fold]}\", f\"{init_folder}/fold_{fold}_weights\")\n",
        "    os.renames(f\"{init_folder}/fold_{fold}/training_metrics.txt\", f\"{init_folder}/fold_{fold}_weights/training_metrics.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j19DYCDn9QrZ"
      },
      "source": [
        "Produce predictions for examples not in the original training set for semi-supervised training.\n",
        "\n",
        "Also produce predictions for the original training set to help with finding the best weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PqeYKdEK9R8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105,
          "referenced_widgets": [
            "f13246ec38d1436fa522fb48ca231062",
            "ab929c2f298c4c238814eb7c8f4cfc4a",
            "96ca34620c574b7e91915bb22582487b",
            "729ffdf802364a3fb7dc61f7400b035d",
            "7c87198d0e784af9aadaeef7325255c1",
            "6134e1ea5e86474085bf009a38390b4f",
            "3b228be289e34aaf8a6c19fd975f7e35",
            "60004c1a9f5b4e26bb9d6035c8e9a7db",
            "90ec28d96c5c4456ad3ba6948a7ec40b",
            "1264b7d97e214d85bbea96b43156ea9b",
            "4c588a0b45b64ce9ae523ddae654b75f"
          ]
        },
        "outputId": "575ceaab-5cd5-4a3f-a047-db4f84e38a13"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/148 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f13246ec38d1436fa522fb48ca231062"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:163: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  arr = asanyarray(a)\n"
          ]
        }
      ],
      "source": [
        "datasets = [\"semi_sup\", \"original\"]\n",
        "\n",
        "for dataset in datasets:\n",
        "    if dataset == \"semi_sup\":\n",
        "        # All samples we have not classified so far, this is something that needs to be changed for the submission test set\n",
        "        test_samples = pd.merge(train_dataset, train_df, on=['case_num', 'pn_num', 'feature_num'], how='outer', indicator=True) \\\n",
        "            .query(\"_merge != 'both'\") \\\n",
        "            .drop('_merge', axis=1) \\\n",
        "            .reset_index(drop=True)\n",
        "\n",
        "        test_samples.drop(columns=['feature_text', 'pn_history', 'location'], inplace=True)\n",
        "    else:\n",
        "        test_samples = pd.merge(train_dataset, train_df, on=['case_num', 'pn_num', 'feature_num'], how='inner')\n",
        "        test_samples.drop(columns=['feature_text', 'pn_history'], inplace=True)\n",
        "\n",
        "    # The Kaggle Public Model uses a different max length and model architecture so must be treated differently\n",
        "    test_dataset = TestDataset(train_dataset, test_samples, cur_tokenizer, public_kaggle_model=(\"kaggle\" in ModelParams.model_type))\n",
        "    test_loader = DataLoader(dataset=test_dataset, batch_size=ModelParams.eval_batch_size, num_workers=ModelParams.num_workers, pin_memory=ModelParams.pin_mem)\n",
        "\n",
        "    softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    progress_bar = tqdm(range(len(test_loader)))\n",
        "\n",
        "    predictions = []\n",
        "    for fold in range(ModelParams.n_folds):\n",
        "        if ModelParams.model_type == 'roberta':\n",
        "            init_model = TokenModel('roberta-large')\n",
        "        elif ModelParams.model_type == 'deberta':\n",
        "            init_model = TokenModel('microsoft/deberta-v3-large')\n",
        "        else:\n",
        "            init_model = ScoringModel(config_path=init_folder+'/config.pth', pretrained=False)\n",
        "\n",
        "        if \"kaggle\" in ModelParams.model_type: init_model.load_state_dict(torch.load(f\"{init_folder}/deberta-v3-large_fold{fold}_best.pth\")['model'])\n",
        "        else: init_model.load_state_dict(torch.load(f\"{init_folder}/fold_{fold}_weights/model.pth\"))\n",
        "\n",
        "        for param_group in init_model.parameters():\n",
        "            param_group.data = param_group.data.to(torch.float16)\n",
        "        \n",
        "        preds = []\n",
        "        offset_mappings = []\n",
        "        test_texts = []\n",
        "        init_model.eval()\n",
        "        init_model.to(device)\n",
        "\n",
        "        for inputs in test_loader:\n",
        "            offset_mappings.append(inputs['offset_mapping'].numpy())\n",
        "            test_texts += inputs['text']\n",
        "            del inputs['offset_mapping'], inputs['text']\n",
        "            inputs = {k: v.to(device, non_blocking=True) for k, v in inputs.items()}\n",
        "            with torch.no_grad():\n",
        "                y_preds = init_model(inputs)\n",
        "                if \"kaggle\" in ModelParams.model_type: batch_pred = y_preds.sigmoid().to('cpu').numpy()\n",
        "                else: batch_pred = softmax(y_preds)[:, 1, :].to('cpu').numpy()\n",
        "                preds.append(batch_pred)\n",
        "            progress_bar.update(1)\n",
        "        offset_mappings = np.concatenate(offset_mappings)\n",
        "        token_prediction = np.concatenate(preds)\n",
        "\n",
        "        if \"kaggle\" in ModelParams.model_type: length = 354\n",
        "        else: length = ModelParams.max_length\n",
        "\n",
        "        token_prediction = token_prediction.reshape((len(test_samples), length))\n",
        "        char_probs = get_char_probs(test_texts, token_prediction, offset_mappings)\n",
        "        predictions.append(char_probs)\n",
        "        del init_model, token_prediction, char_probs\n",
        "        torch.cuda.empty_cache()\n",
        "        \n",
        "    predictions = np.mean(predictions, axis=0)\n",
        "    test_samples[\"predictions\"] = predictions\n",
        "    test_samples[\"text\"] = test_texts\n",
        "    if dataset == \"semi_sup\": test_samples.to_csv(f\"{init_folder}/predictions.csv\", index=False)  \n",
        "    else: test_samples.to_csv(f\"{init_folder}/train_predictions.csv\", index=False)  "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Look at predictions for developing threshold score values for the different models (optional)"
      ],
      "metadata": {
        "id": "0wcoeOJvR_4s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_threshold_cell = False\n",
        "\n",
        "if run_threshold_cell:\n",
        "    train_predictions = pd.read_csv(f\"{init_folder}/train_predictions.csv\")\n",
        "    np.set_printoptions(threshold=10_000_000, precision=4, suppress=True)\n",
        "    pred_df = train_predictions[[\"predictions\"]]\n",
        "    pred_df[\"predictions\"] = pred_df[\"predictions\"].apply(lambda x: [float(i) for i in x[1:-1].split()])\n",
        "\n",
        "    orig_train = train_dataset.merge(train_df, how='inner', on=['case_num', 'pn_num', 'feature_num'])\n",
        "    orig_train.reset_index(inplace=True, drop=True)\n",
        "\n",
        "    true_labels_and_preds = pd.concat([orig_train, pred_df], axis=1)\n",
        "    def flatten(t):\n",
        "        return [item for sublist in t for item in sublist]\n",
        "\n",
        "    print(f\"Mean prediction over all classes: {np.mean(np.array(pred_df['predictions']))}\")\n",
        "    true_labels_and_preds[\"the_prds\"] = true_labels_and_preds.apply(lambda x: flatten([x[\"predictions\"][span[0]:span[1]] for span in location_list(x[\"location\"])]), axis=1)\n",
        "\n",
        "    # COMMENT above\n",
        "    n_bins = 50\n",
        "    fig, axs = plt.subplots(1, 2, sharey=True, tight_layout=True)\n",
        "\n",
        "    dist1 = flatten(list(true_labels_and_preds[\"the_prds\"]))\n",
        "    #dist2 = \n",
        "    axs[0].hist(dist1, bins=n_bins)\n",
        "    # axs[1].hist(dist2, bins=n_bins)\n",
        "    fig.show()\n",
        "\n",
        "    df = pd.DataFrame({\"numbers\": flatten(list(true_labels_and_preds[\"the_prds\"]))})\n",
        "    print(df.describe())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "id": "oVsOeiKOvgNu",
        "outputId": "6c34d80a-0f79-4cf3-c936-03c8015c66ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean prediction over all classes: [0.0001 0.0001 0.0001 ... 0.0001 0.0001 0.0001]\n",
            "             numbers\n",
            "count  199989.000000\n",
            "mean        0.713145\n",
            "std         0.243774\n",
            "min         0.000000\n",
            "25%         0.795410\n",
            "50%         0.796387\n",
            "75%         0.796875\n",
            "max         0.811523\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ar-RVhU9YN-"
      },
      "source": [
        "# Generate word level labels (locations) given predictions from multiple models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5AJWPA29abg"
      },
      "outputs": [],
      "source": [
        "predictions = []\n",
        "weight_averages = {'roberta': 0.7944,\n",
        "                   'deberta': 0.787,\n",
        "                   'deberta-kaggle': 0.5}\n",
        "\n",
        "for model_type, w in weight_averages.items():\n",
        "\tmax_len_preds, _ = produce_same_len_predictions(model_type, \"train\")\n",
        "    predictions.append(max_len_preds*0.5/w)\n",
        "\n",
        "different_weights = []\n",
        "\n",
        "for rob_w in np.arange(0.05, 0.33, 0.01):\n",
        "    for deb_w in np.arange(0.33, 0.45, 0.01):\n",
        "        debk_w = 1-rob_w-deb_w\n",
        "        different_weights.append((rob_w, deb_w, debk_w))\n",
        "\n",
        "weights_and_scores = []\n",
        "\n",
        "for weights in different_weights:\n",
        "    wtd_pred = [predictions[i]*weights[i] for i in range(3)]\n",
        "    wtd_pred = np.sum(wtd_pred, axis=0)\n",
        "\n",
        "    locations = np.array(train_df.apply(lambda x: get_char_locations(x[\"location\"], x[\"pn_history\"]), axis=1))\n",
        "    true_positive = np.sum(locations*(wtd_pred >= 0.5))\n",
        "    false_positive = np.sum((1-locations)*(wtd_pred >= 0.5))\n",
        "    false_negative = np.sum(locations*(wtd_pred < 0.5))\n",
        "    weight_f1 = true_positive / (true_positive + 0.5*(false_positive+false_negative))\n",
        "    weights_and_scores.append((weight_f1, weights))\n",
        "\n",
        "# Print 5 best weights\n",
        "weights_and_scores = sorted(weights_and_scores, key=(lambda x: x[0]), reverse=True)\n",
        "print(weights_and_scores[:5])\n",
        "\n",
        "\n",
        "# Now with the best weights, produce predictions\n",
        "best_model_weights = {'roberta': 0.1,\n",
        "                      'deberta': 0.4,\n",
        "                      'deberta-kaggle': 0.5}\n",
        "\n",
        "predictions = []\n",
        "for model_type, w in best_model_weights.items():\n",
        "    max_len_preds, prediction_df = produce_same_len_predictions(model_type, \"semi-sup\")\n",
        "    predictions.append(max_len_preds*w*0.5/weight_averages[model_type])\n",
        "\n",
        "predictions = np.sum(predictions, axis=0)\n",
        "prediction_df[\"predictions\"] = predictions\n",
        "semi_supervised_submissions = get_results(prediction_df)\n",
        "\n",
        "# ONLY DO THIS for producing semi-supervised submissions as need gold labels during training\n",
        "semi_supervised_submissions[\"gold\"] = False\n",
        "gold_df = train_df[[\"id\"]]\n",
        "gold_df[\"location\"] = train_df[\"location\"].apply(lambda x: ';'.join([f\"{loc[0]} {loc[1]}\" for loc in location_list(x)]))\n",
        "gold_df[\"gold\"] = True\n",
        "semi_supervised_submissions = semi_supervised_submissions.append(gold_df, ignore_index=True)\n",
        "\n",
        "semi_supervised_submissions.to_csv('../public_models/semi_supervised_submission.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXLmw1U99i2R"
      },
      "source": [
        "# Fine-tuning our model on the semi-supervised training set"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "semi_supervised_submission = pd.read_csv('../public_models/semi_supervised_submission.csv')\n",
        "semi_supervised_submission[\"pn_num\"] = semi_supervised_submission[\"id\"].apply(lambda x: int(x[:5]))\n",
        "semi_supervised_submission[\"feature_num\"] = semi_supervised_submission[\"id\"].apply(lambda x: int(x[6:]))\n",
        "semi_supervised_submission[\"case_num\"] = semi_supervised_submission[\"id\"].apply(lambda x: int(x[6:7]))\n",
        "semi_supervised_submission[\"location\"] = semi_supervised_submission[\"location\"].apply(lambda x: x.split(';'))\n",
        "semi_supervised_submission = create_k_fold_col(semi_supervised_submission)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss(ce_weights, reduction='sum')\n",
        "\n",
        "for fold in range(ModelParams.n_folds):\n",
        "    if ModelParams.model_type == 'roberta':\n",
        "        fin_model = TokenModel('roberta-large')\n",
        "    elif ModelParams.model_type == 'deberta':\n",
        "        fin_model = TokenModel('microsoft/deberta-v3-large')\n",
        "\n",
        "    fin_model.load_state_dict(torch.load(f\"{init_folder}/fold_{fold}_weights/model.pth\"))\n",
        "\n",
        "    # move the model to device\n",
        "    fin_model.to(device)\n",
        "    # Train Model\n",
        "    fin_model.train()\n",
        "\n",
        "    # Output folder for model weights\n",
        "    fold_folder = f\"{fin_folder}/fold_{fold}\"\n",
        "    if not os.path.exists(fold_folder): os.makedirs(fold_folder)\n",
        "\n",
        "    train_metrics_path = f\"{fold_folder}/training_metrics.txt\"\n",
        "\n",
        "    fin_fold_gold_df = semi_supervised_submission[semi_supervised_submission['fold'] != fold][semi_supervised_submission['gold'] == True].copy(deep=True)\n",
        "    fin_eval_gold_df = semi_supervised_submission[semi_supervised_submission['fold'] == fold][semi_supervised_submission['gold'] == True].copy(deep=True)\n",
        "\n",
        "    fin_fold_silver_df = semi_supervised_submission[semi_supervised_submission['fold'] != fold][semi_supervised_submission['gold'] == False].copy(deep=True)\n",
        "    fin_eval_silver_df = semi_supervised_submission[semi_supervised_submission['fold'] == fold][semi_supervised_submission['gold'] == False].copy(deep=True)\n",
        "\n",
        "    # how many times the training dataset we want to use for semi-supervised training, cannot use entire set due to runtime restrictions\n",
        "    n_times_train_dataset = 6\n",
        "    fin_fold_df = pd.concat([fin_fold_gold_df, fin_fold_silver_df.sample \\\n",
        "                             (n=len(fin_fold_gold_df)*(n_times_train_dataset-1), random_state=ModelParams.rand_seed)])\n",
        "    fin_eval_df = pd.concat([fin_eval_gold_df, fin_eval_silver_df.sample \\\n",
        "                             (n=len(fin_eval_gold_df)*(n_times_train_dataset-1), random_state=ModelParams.rand_seed)])\n",
        "\n",
        "    fin_train_dataset = PatientNotesDataset(train_dataset, fin_fold_df, cur_tokenizer)\n",
        "    fin_eval_dataset = PatientNotesDataset(train_dataset, fin_eval_df, cur_tokenizer)\n",
        "    fin_train_dataloader = DataLoader(dataset=fin_train_dataset, fin_batch_size=ModelParams.fin_batch_size, shuffle=True, num_workers=ModelParams.num_workers, pin_memory=ModelParams.pin_mem)\n",
        "    fin_train_eval_dataloader = DataLoader(dataset=fin_train_dataset, fin_batch_size=ModelParams.eval_batch_size, num_workers=ModelParams.num_workers, pin_memory=ModelParams.pin_mem)\n",
        "    fin_eval_dataloader = DataLoader(dataset=fin_eval_dataset, fin_batch_size=ModelParams.eval_batch_size, num_workers=ModelParams.num_workers, pin_memory=ModelParams.pin_mem)\n",
        "\n",
        "    # Optimizer and schedules for optimizer\n",
        "    fin_optimizer = AdamW(fin_model.parameters(), weight_decay=0.01)\n",
        "    fin_optimizer.load_state_dict(torch.load(f\"{init_folder}/fold_{fold}_weights/optim.pth\"))\n",
        "\n",
        "    base_lr=2e-6      # Might need higher learning rate for here, WARNING\n",
        "    num_warmup_steps=6000\n",
        "\n",
        "    num_epochs = ModelParams.num_epochs\n",
        "    num_training_steps = num_epochs * len(fin_train_dataloader)\n",
        "    num_steps = 0\n",
        "\n",
        "    best_val_loss = np.inf\n",
        "    best_val_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    print(f\"Fold {fold}:\\n\")\n",
        "    progress_bar = tqdm(range(num_training_steps))\n",
        "    row_format =\"{:<10} {:<20} {:<15} {:<20} {:<15}\"\n",
        "    now = datetime.now()\n",
        "    dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
        "    write_to_file(train_metrics_path, f\"Run on {dt_string}. LR = {base_lr}, Epochs = {ModelParams.num_epochs}\\n\")\t\n",
        "    write_to_file(train_metrics_path, row_format.format(\"\", \"Approx. train loss\", \"Val loss\", \"Approx. train f1\", \"Val f1\") + '\\n')\n",
        "\n",
        "    for epoch in range(ModelParams.num_epochs):\n",
        "        fin_optimizer.param_groups[0]['lr'] = fin_optimizer.param_groups[0]['lr']*0.65\n",
        "        for fin_batch in fin_train_dataloader:\n",
        "            if num_steps < num_warmup_steps:\n",
        "                fin_optimizer.param_groups[0]['lr'] = base_lr*(num_steps+1)/num_warmup_steps\n",
        "                num_steps+=1\n",
        "\n",
        "            num_in_batch = len(fin_batch['input_ids'])\n",
        "            fin_batch = {k: v.to(device, non_blocking=True) for k, v in fin_batch.items()}\n",
        "            labels = fin_batch['labels']\n",
        "            del fin_batch['labels']\n",
        "            outputs = init_model(fin_batch)\n",
        "            loss = loss_fn(outputs, labels)/num_in_batch\n",
        "            loss.backward()\n",
        "\n",
        "            del fin_batch, labels, outputs, loss\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "            fin_optimizer.step()\n",
        "            fin_optimizer.zero_grad(set_to_none=True)\n",
        "            progress_bar.update(1)\n",
        "\n",
        "        # Every epoch print metrics, save if necessary\n",
        "        fin_model.eval()\n",
        "        with torch.no_grad():\n",
        "            train_tp, train_fp, train_fn = 0, 0, 0\n",
        "\n",
        "            # compute train loss\n",
        "            train_loss = 0\n",
        "            rand_loss_iters = set(random.sample(range(len(fin_train_eval_dataloader)), len(fin_train_eval_dataloader)//10)) # eval 0.1 of training set for approx.\n",
        "            \n",
        "            for count, fin_batch in enumerate(fin_train_eval_dataloader):\n",
        "                if count not in rand_loss_iters: continue\n",
        "\n",
        "                fin_batch = {k: v.to(device, non_blocking=True) for k, v in fin_batch.items()}\n",
        "                labels = fin_batch['labels']\n",
        "                del fin_batch['labels']\n",
        "\n",
        "                outputs = fin_model(fin_batch)\n",
        "                train_loss += loss_fn(outputs, labels)/len(rand_loss_iters)\n",
        "\n",
        "                predictions = torch.argmax(outputs, dim=1)\n",
        "                f1_metrics = calculate_f1_metrics(predictions=predictions, references=labels, attention_mask=fin_batch['attention_mask'])\n",
        "                train_tp += f1_metrics[0]\n",
        "                train_fp += f1_metrics[1]\n",
        "                train_fn += f1_metrics[2]\n",
        "\n",
        "            val_tp, val_fp, val_fn = 0, 0, 0\n",
        "\n",
        "            # compute val loss and metrics\n",
        "            val_loss = 0\n",
        "            for fin_batch in fin_eval_dataloader:\n",
        "                fin_batch = {k: v.to(device, non_blocking=True) for k, v in fin_batch.items()}\n",
        "                labels = fin_batch['labels']\n",
        "                del fin_batch['labels']\n",
        "\n",
        "                outputs = fin_model(fin_batch)\n",
        "                val_loss += loss_fn(outputs, labels)/len(fin_eval_dataloader)\n",
        "\n",
        "                predictions = torch.argmax(outputs, dim=1)\n",
        "                f1_metrics = calculate_f1_metrics(predictions=predictions, references=labels, attention_mask=fin_batch['attention_mask'])\n",
        "                val_tp += f1_metrics[0]\n",
        "                val_fp += f1_metrics[1]\n",
        "                val_fn += f1_metrics[2]\n",
        "\n",
        "            del fin_batch, outputs, labels, predictions, f1_metrics\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "            if train_tp == 0 and train_fp == 0 and train_fn == 0: train_f1 = -1   # -1 as in weird input/error\n",
        "            else: train_f1 = train_tp / (train_tp + 0.5*(train_fp+train_fn))\n",
        "            if val_tp == 0 and val_fp == 0 and val_fn == 0: val_f1 = -1\n",
        "            else: val_f1 = val_tp / (val_tp + 0.5*(val_fp+val_fn))\n",
        "            write_to_file(train_metrics_path, row_format.format(f\"Epoch {epoch+1}\", f\"{train_loss:.6f}\", f\"{val_loss:.6f}\", f\"{train_f1:.6f}\", f\"{val_f1:.6f}\") + '\\n')\n",
        "\n",
        "            val_losses.append(val_loss)\n",
        "\n",
        "            if epoch >= 7 and val_loss > (val_losses[epoch-3] + val_losses[epoch-2]*0.9 \\\n",
        "                                            + val_losses[epoch-1]*0.9**2)/(1+0.9+0.9**2):\n",
        "                write_to_file(train_metrics_path, \"\\nValidation loss stopped decreasing compared to weighted average of last three epochs, stopping.\\n\")\n",
        "                break\n",
        "            \n",
        "            if epoch >= 1 and val_loss < val_losses[epoch-1] and val_loss < 1.05*best_val_loss:\n",
        "                best_val_loss = only_save_best_models(best_val_losses, epoch, val_loss, best_val_loss, fold_folder, fin_model)\n",
        "        fin_model.train()\n",
        "\n",
        "    for k in keys:\n",
        "        if k.startswith('fin_'): del globals()[k]\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "\n",
        "    # WHEN ACTUAL TRAINING CAN WATCH THEN SET NUM EPOCHS AFTER FIRST FOLD"
      ],
      "metadata": {
        "id": "bURLpTjC0DHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After we decide on the best epoch numbers for each fold, keep only the weights for the best epoch"
      ],
      "metadata": {
        "id": "e8Su5Wz44Joo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manual Input\n",
        "best_epochs = {0: 1,\n",
        "               1: 0,\n",
        "               2: 0,\n",
        "               3: 0,\n",
        "               4: 0}\n",
        "\n",
        "# For all folds keep only the best epoch\n",
        "for fold in range(ModelParams.n_folds):\n",
        "    fold_folder = f\"{fin_folder}/fold_{fold}\"\n",
        "    del_epochs = [folder.name for folder in os.scandir(fold_folder) if folder.is_dir()]\n",
        "    del_epochs.remove(f\"epoch_{best_epochs[fold]}\")\n",
        "\n",
        "    for del_epoch in del_epochs: shutil.rmtree(f\"{fin_folder}/fold_{fold}/{del_epoch}\")   # remove all directories not containing best epoch\n",
        "\n",
        "    os.renames(f\"{fin_folder}/fold_{fold}/epoch_{best_epochs[fold]}\", f\"{fin_folder}/fold_{fold}_weights\")\n",
        "    os.renames(f\"{fin_folder}/fold_{fold}/training_metrics.txt\", f\"{fin_folder}/fold_{fold}_weights/training_metrics.txt\")"
      ],
      "metadata": {
        "id": "9M5YoLSP4Ki9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Produce predictions for examples in the training set to find best weights for final model"
      ],
      "metadata": {
        "id": "UE0UU-cOfcRJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_samples = pd.merge(train_dataset, train_df, on=['case_num', 'pn_num', 'feature_num'], how='inner')\n",
        "test_dataset = TestDataset(train_dataset, test_samples, cur_tokenizer)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=ModelParams.eval_batch_size, num_workers=ModelParams.num_workers, pin_memory=ModelParams.pin_mem)\n",
        "\n",
        "progress_bar = tqdm(range(len(test_loader)))\n",
        "\n",
        "predictions = []\n",
        "for fold in range(ModelParams.n_folds):\n",
        "    if ModelParams.model_type == 'roberta':\n",
        "        initial_model = TokenModel('roberta-large')\n",
        "    elif ModelParams.model_type == 'deberta':\n",
        "        initial_model = TokenModel('microsoft/deberta-v3-large')\n",
        "\n",
        "    initial_model.load_state_dict(torch.load(f\"{init_folder}/fold_{fold}_weights/model.pth\"))\n",
        "\n",
        "    for param_group in initial_model.parameters():\n",
        "        param_group.data = param_group.data.to(torch.float16)\n",
        "    \n",
        "    preds = []\n",
        "    offset_mappings = []\n",
        "    test_texts = []\n",
        "    initial_model.eval()\n",
        "    initial_model.to(device)\n",
        "\n",
        "    for inputs in test_loader:\n",
        "        offset_mappings.append(inputs['offset_mapping'].numpy())\n",
        "        test_texts += inputs['text']\n",
        "        del inputs['offset_mapping'], inputs['text']\n",
        "        inputs = {k: v.to(device, non_blocking=True) for k, v in inputs.items()}\n",
        "        with torch.no_grad():\n",
        "            y_preds = initial_model(inputs)\n",
        "            temp = softmax(y_preds)[:, 1, :].to('cpu').numpy()\n",
        "            preds.append(temp)\n",
        "        progress_bar.update(1)\n",
        "    offset_mappings = np.concatenate(offset_mappings)\n",
        "    token_prediction = np.concatenate(preds)\n",
        "\n",
        "    token_prediction = token_prediction.reshape((len(test_samples), ModelParams.max_length))\n",
        "    char_probs = get_char_probs(test_texts, token_prediction, offset_mappings)\n",
        "    predictions.append(char_probs)\n",
        "    del initial_model, token_prediction, char_probs\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "predictions = np.mean(predictions, axis=0)\n",
        "test_samples[\"predictions\"] = predictions\n",
        "test_samples[\"text\"] = test_texts\n",
        "test_samples.to_csv(f\"{fin_folder}/train_predictions.csv\", index=False)  "
      ],
      "metadata": {
        "id": "mZnZLEk1foej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finding the best weights for the final model"
      ],
      "metadata": {
        "id": "R4wN64BegvNg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models = ['roberta', 'deberta', 'deberta-kaggle']\n",
        "\n",
        "predictions = []\n",
        "for model_type in models:\n",
        "    max_len_preds, prediction_df = produce_same_len_predictions(model_type, \"semi-sup\", training_section=\"final\")\n",
        "    predictions.append(thing)\n",
        "\n",
        "different_weights = []\n",
        "for rob_w in np.arange(0.05, 0.33, 0.01):\n",
        "    for deb_w in np.arange(0.33, 0.45, 0.01):\n",
        "        debk_w = 1-rob_w-deb_w\n",
        "        different_weights.append((rob_w, deb_w, debk_w))\n",
        "\n",
        "weights_and_scores = []\n",
        "\n",
        "for weights in different_weights:\n",
        "    new_pred = [predictions[i]*weights[i] for i in range(3)]\n",
        "    new_pred = np.sum(new_pred, axis=0)\n",
        "\n",
        "    locations = np.array(train_df.apply(lambda x: get_char_locations(x[\"location\"], x[\"pn_history\"]), axis=1))\n",
        "    true_positive = np.sum(locations*(new_pred >= 0.5))\n",
        "    false_positive = np.sum((1-locations)*(new_pred >= 0.5))\n",
        "    false_negative = np.sum(locations*(new_pred < 0.5))\n",
        "    weight_f1 = true_positive / (true_positive + 0.5*(false_positive+false_negative))\n",
        "    weights_and_scores.append((weight_f1, weights))\n",
        "weights_and_scores = sorted(weights_and_scores, key=(lambda x: x[0]), reverse=True)\n",
        "print(weights_and_scores[:5])"
      ],
      "metadata": {
        "id": "gG9iNkoqg0a_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "E4xLzOPNFOSt",
        "hVJUPQMZuTFY"
      ],
      "name": "NBME Generate Models.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f13246ec38d1436fa522fb48ca231062": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ab929c2f298c4c238814eb7c8f4cfc4a",
              "IPY_MODEL_96ca34620c574b7e91915bb22582487b",
              "IPY_MODEL_729ffdf802364a3fb7dc61f7400b035d"
            ],
            "layout": "IPY_MODEL_7c87198d0e784af9aadaeef7325255c1"
          }
        },
        "ab929c2f298c4c238814eb7c8f4cfc4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6134e1ea5e86474085bf009a38390b4f",
            "placeholder": "​",
            "style": "IPY_MODEL_3b228be289e34aaf8a6c19fd975f7e35",
            "value": "100%"
          }
        },
        "96ca34620c574b7e91915bb22582487b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_60004c1a9f5b4e26bb9d6035c8e9a7db",
            "max": 148,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_90ec28d96c5c4456ad3ba6948a7ec40b",
            "value": 148
          }
        },
        "729ffdf802364a3fb7dc61f7400b035d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1264b7d97e214d85bbea96b43156ea9b",
            "placeholder": "​",
            "style": "IPY_MODEL_4c588a0b45b64ce9ae523ddae654b75f",
            "value": " 148/148 [01:08&lt;00:00,  2.77it/s]"
          }
        },
        "7c87198d0e784af9aadaeef7325255c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6134e1ea5e86474085bf009a38390b4f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b228be289e34aaf8a6c19fd975f7e35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "60004c1a9f5b4e26bb9d6035c8e9a7db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90ec28d96c5c4456ad3ba6948a7ec40b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1264b7d97e214d85bbea96b43156ea9b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c588a0b45b64ce9ae523ddae654b75f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}