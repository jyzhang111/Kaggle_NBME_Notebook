{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports and Utils","metadata":{"id":"yjeJLN0bUIpS"}},{"cell_type":"markdown","source":"Fast tokenizer borrowed from Thanh Nguyen's Fast Tokenizer with files [here](https://www.kaggle.com/datasets/thanhns/deberta-v2-3-fast-tokenizer) and [here](https://www.kaggle.com/datasets/thanhns/deberta-tokenizer).","metadata":{"id":"Juv2fVo_UIpc"}},{"cell_type":"code","source":"# The following is necessary if you want to use the fast tokenizer for deberta v2 or v3\n# This must be done before importing transformers\nimport shutil\nfrom pathlib import Path\nimport transformers\n\ntransformers_path = Path(transformers.__file__.replace(\"/__init__.py\", \"\"))\n\ninput_dir = Path(\"../input/deberta-v2-3-fast-tokenizer\")\n\nconvert_file = input_dir / \"convert_slow_tokenizer.py\"\nconversion_path = transformers_path/convert_file.name\n\nif conversion_path.exists():\n    conversion_path.unlink()\n\nshutil.copy(convert_file, transformers_path)\ndeberta_v2_path = transformers_path / \"models\" / \"deberta_v2\"\n\nfor filename in ['tokenization_deberta_v2.py', 'tokenization_deberta_v2_fast.py']:\n    filepath = deberta_v2_path/filename\n    \n    if filepath.exists():\n        filepath.unlink()\n\n    shutil.copy(input_dir/filename, filepath)         ","metadata":{"id":"9tfn8mIZUIpe","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport gc\nimport sys\nimport ast\nimport random\nfrom datetime import datetime\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1300)\nimport matplotlib.pyplot as plt\nfrom tqdm.auto import tqdm\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import train_test_split\n\nfrom transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast\nfrom transformers import RobertaTokenizerFast, AutoModel, AutoConfig\nfrom transformers import AdamW, get_scheduler\nfrom torch.utils.data import DataLoader, Dataset\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n%env TOKENIZERS_PARALLELISM=true\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"id":"dWO81M6nUIpk","outputId":"5646e9cd-17ba-4c68-b6e2-817016eda1e6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ModelParams:\n    pt_batch_size = 4      # Batch size when pre-training with MLM, as transformer layers are trained, GPU RAM can only handle batch sizes of 4\n    init_batch_size = 16    # For when model has just our beginning supervised data\n    fin_batch_size = 32     # For when model has semi-supervised data\n    eval_batch_size = 40     # For model evaluation, (no backward pass)\n    max_length = 460        # Max length found from previous tokenizations\n    rand_seed = 42\n    num_workers = 0\n    pin_mem = True if \\\n      num_workers > 0 else False\n    num_epochs = 12\n    n_folds = 5             # Num folds for cross validation\n#     model_type = 'roberta'  # Fine tune the different models on separate notebooks","metadata":{"id":"EjSXLlxUUIpl","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed=0):\n    '''\n    Sets the seed of the entire notebook so results are the same every time we run.\n    This is for REPRODUCIBILITY.\n    '''\n    random.seed(seed)\n    # Set a fixed value for the hash seed\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    \n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        # When running on the CuDNN backend, two further options must be set\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n\nseed_everything(seed=ModelParams.rand_seed)","metadata":{"id":"CdlOrv8_UIpm","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset creation","metadata":{"id":"KM0cx_fdUIpn"}},{"cell_type":"markdown","source":"Data helper functions","metadata":{"id":"TQ8nzNZVUIpn"}},{"cell_type":"code","source":"def print_gpu_memory():\n    info = nvmlDeviceGetMemoryInfo(H)\n    print(f'total    : {info.total}')\n    print(f'free     : {info.free}')\n    print(f'used     : {info.used}')\n\ndef write_to_file(file_path, print_string, option='a'):\n    # Need to write to file in this manner due to Colab's synchronization with Drive\n    f = open(file_path, option)\n    f.write(print_string)\n    f.close()\n    del f\n    gc.collect()\n\ndef only_save_best_models(best_losses, epoch, loss, best_loss, fold_folder, model, optimizer=None):\n    '''\n    Function for managing data storage. Given the inputs, keep only the three best models\n    in storage in terms of the loss (train_loss or val_loss).\n\n    Returns: min(loss, best_loss)\n    '''\n    best_losses.append((epoch, loss))\n    best_losses.sort(key=lambda x: x[1])\n    epoch_folder = f\"{fold_folder}/epoch_{epoch+1}\"\n    # Only want to save three best models to save memory\n    if len(best_losses) > 3:\n        popped = best_losses.pop(3)\n        if popped[0] != epoch:\n            shutil.rmtree(f\"{fold_folder}/epoch_{popped[0]+1}\")\n            if not os.path.exists(epoch_folder): os.makedirs(epoch_folder)\n            torch.save(model.state_dict(), epoch_folder+\"/model.pth\")\n            if optimizer is not None: torch.save(optimizer.state_dict(), epoch_folder+\"/optim.pth\")\n    else:\n        if not os.path.exists(epoch_folder): os.makedirs(epoch_folder)\n        torch.save(model.state_dict(), epoch_folder+\"/model.pth\")\n        if optimizer is not None: torch.save(optimizer.state_dict(), epoch_folder+\"/optim.pth\")\n    return min(loss, best_loss)\n\ndef create_k_fold_col(train_df):\n    skf = StratifiedKFold(n_splits=ModelParams.n_folds, shuffle=True, random_state=ModelParams.rand_seed)\n    if \"gold\" in train_df.columns:\n        train_df[\"stratify_on\"] = train_df[\"case_num\"].astype(str) + train_df[\"feature_num\"].astype(str) + train_df[\"gold\"].astype(str)\n    else:\n        train_df[\"stratify_on\"] = train_df[\"case_num\"].astype(str) + train_df[\"feature_num\"].astype(str)\n    train_df[\"fold\"] = -1      # Create a column to remember the fold\n    for fold, (_, valid_idx) in enumerate(skf.split(np.zeros(len(train_df)), y=train_df[\"stratify_on\"])):\n        train_df.loc[valid_idx, \"fold\"] = fold\n    return train_df\n\ndef location_list(string_locs):\n    # example: ['0 1', '3 4', '4 4;5 6'] -> [(0, 1), (3, 4), (4, 4), (5, 6)]\n    loc_list = []\n    for loc_string in string_locs:\n        for section in loc_string.split(';'):\n            start, end = section.split()\n            loc_list.append((int(start), int(end)))\n    return loc_list\n\ndef calculate_f1_metrics(predictions, references, attention_mask):\n    '''\n    Given a batch of predictions, references, and their attention_masks, return\n    the f1 metrics true_positive, false_positive, and false_negative from the batch.\n    \n    Args:\n        predictions (tensor - list of list of ints): Predictions for each token in each sentence of the batch\n        references (tensor - list of list of ints): Labels for each token in each sentence of the batch\n        attention_mask (tensor - list of list of ints): Attention mask for tokens in the batch\n\n    Returns:\n        true_positive, false_positive, false_negative: ints for each of these values (wrt desired label 1)\n    '''\n    predictions = predictions * attention_mask\n    references = references * attention_mask\n    true_positive = torch.sum(predictions*references) + 0.1\n    false_positive = torch.sum(predictions*(1-references)) + 0.1\n    false_negative = torch.sum((1-predictions)*references) + 0.1\n    return int(true_positive), int(false_positive), int(false_negative)\n\ndef convert_to_labels(offset_mapping, loc_list):\n    '''\n    Given a bunch of offset_mappings, return a tensor with label 1 for each token\n    contained within the boundaries of a location in location_list.\n    \n    Args:\n        offset_mapping (list of tuples of two ints): Token spans.\n        loc_list (list of tuples of two ints): Char spans in sentence with positive feature label.\n\n    Returns:\n        torch tensor [ModelParams.max_length]: Binarized token label.\n    '''\n    labels = torch.zeros(ModelParams.max_length, dtype=torch.int64)\n    if len(loc_list) == 0: return labels\n    for i in range(ModelParams.max_length):\n        if offset_mapping[i][0] >= loc_list[0][0]-1 and offset_mapping[i][1] <= loc_list[-1][1]:\n            for loc in loc_list:\n                if offset_mapping[i][0] >= loc[0]-1:\n                    if offset_mapping[i][1] <= loc[1]:\n                        labels[i] = 1\n                else: break\n    return labels\n\ndef get_char_probs(texts, predictions, offset_mappings):\n    '''\n    From the probabilities for individual tokens, generate probabilities for each\n    of the characters using the offset mappings.\n    '''\n    results = [np.zeros(len(t)) for t in texts]\n    for i, (prediction, offset_mapping) in enumerate(zip(predictions, offset_mappings)):\n        prediction = prediction[:len(offset_mapping)]\n        for idx, (pred, offset_tuple) in enumerate(zip(prediction, offset_mapping)):\n            start = offset_tuple[0]\n            end = offset_tuple[1]\n            results[i][start:end] = pred\n    return results\n\ndef get_results(prediction, th=0.5):\n    '''\n    From the probabilities for individual characters, generate locations for words.\n    '''\n\n    def return_word_spans(text): # _Standard_ tokenization function for returning spans of individual words and punctuation\n        good_punctuation = set(';')\n        punctuation = set([',', '.', '?', '!', '/', ':', '-', '>', '<', '~', '[', ']', '_', ')', '*', '\\\\', '=', '(', '&', '#', '$', '@', '+', '\\n', '\\t'])\n        word_spans = []\n        prev_punc = -1\n        for i in range(len(text)):\n            if text[i] in punctuation:\n                if (i - prev_punc) != 1: word_spans.append((prev_punc+1, i))\n                word_spans.append((i, i+1))\n                prev_punc = i\n            elif text[i].isspace() or text[i] in good_punctuation:\n                if (i - prev_punc) != 1: word_spans.append((prev_punc+1, i))\n                prev_punc = i\n            elif text[i] == '\"':\n                if i+1 != len(text) and text[i+1] == '.': continue\n                if (i - prev_punc) != 1: word_spans.append((prev_punc+1, i))\n                prev_punc = i\n            elif text[i] == \"'\" and not (i-1 >= 0 and text[i-1].isalpha() and i+1 < len(text) and text[i+1].isalpha()):\n                if (i - prev_punc) != 1: word_spans.append((prev_punc+1, i))\n                word_spans.append((i, i+1))\n                prev_punc = i\n        return word_spans\n\n    def merge_spans(predictions, word_spans): # allows word spans to include spaces by merging consecutive word spans that are both inside the label\n        results = []\n        prev_word_inside = False\n        for word in word_spans:\n            label_prob = np.mean(predictions[word[0]:word[1]])\n            if label_prob >= th: \n                if prev_word_inside: results.append((results.pop()[0], word[1]))\n                else: results.append(word)\n                prev_word_inside = True\n            else: prev_word_inside = False\n        results = [f\"{tup[0]} {tup[1]}\" for tup in results]\n        results = \";\".join(results)\n        return results\n\n    prediction[\"word_spans\"] = prediction[\"text\"].apply(return_word_spans)\n    prediction[\"location\"] = prediction.apply(lambda x: merge_spans(x[\"predictions\"], x[\"word_spans\"]), axis=1)\n    prediction[\"id\"] =  prediction.apply(lambda x: str(x[\"pn_num\"]).rjust(5, '0') + '_' + str(x[\"feature_num\"]).rjust(3, '0'), axis=1)\n    prediction = prediction[['id', 'location']]\n\n    return prediction\n\ndef get_char_locations(x, text):\n    # Gold character labels as a numpy array (len of text)\n    actual_locations = np.zeros(len(text))\n    for offset_tuple in location_list(x):\n        start = offset_tuple[0]\n        end = offset_tuple[1]\n        actual_locations[i][start:end] = 1\n    return actual_locations\n\ndef produce_same_len_predictions(model_type, dataset, training_section=\"initial\")\n    # Produce similar length predictions as public Kaggle model has a different max_length\n\tprefix = \"\"\n\tif dataset == \"train\": prefix = \"train_\"\n\n\tif 'kaggle' not in model_type:\n    \tprediction_df = pd.read_csv(f\"../public_models/{model_type}_{training_section}/{prefix}predictions.csv\")\n    \tprediction_df[\"predictions\"] = prediction_df[\"predictions\"].apply(lambda x: np.array([float(i) for i in x[1:-1].split()]))\n    \tmax_len_preds = np.array(prediction_df[\"predictions\"])\n    else:\n        prediction_df = pd.read_csv(f\"../public_models/deberta-v3-large-5-folds-public/{prefix}predictions.csv\")\n        prediction_df[\"predictions\"] = prediction_df[\"predictions\"].apply(lambda x: np.array([float(i) for i in x[1:-1].split()]))\n        max_len_preds = list(prediction_df[\"predictions\"])\n        for index, row in enumerate(predictions[0]):\n            newlen = len(row) - len(max_len_preds[index])\n            max_len_preds[index] = np.concatenate([max_len_preds[index], np.ones(newlen)*0.335]) # 0.335 is ~ threshold value for Kaggle predictions\n    return max_len_preds, prediction_df","metadata":{"id":"FzHf4sW9UIpo","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dataset annotation error cleanup","metadata":{"id":"E4xLzOPNFOSt"}},{"cell_type":"code","source":"# This function just cleans up some of the clear annotation errors\ndef clean_datasets(symptoms, pn_notes, train_df):\n    symptoms.loc[27, 'feature_text'] = \"last pap smear 1 year ago\"\n\n# When training these lines were run\n#     train_df.loc[[338], 'location'] = '[\"764 783\"]'\n#     train_df.loc[[621], 'location'] = '[\"77 100\", \"398 420\"]'\n#     train_df.loc[[655], 'location'] = '[\"285 292;301 312\", \"285 287;296 312\"]'\n#     train_df.loc[[1262], 'location'] = '[\"551 557;565 580\"]'\n#     train_df.loc[[1265], 'location'] = '[\"131 135;181 212\"]'\n#     train_df.loc[[1424], 'location'] = '[\"74 88;109 129\"]'\n#     train_df.loc[[1591], 'location'] = '[\"176 184;201 212\"]'\n#     train_df.loc[[1615], 'location'] = '[\"249 257;271 288\"]'\n#     train_df.loc[[1664], 'location'] = '[\"822 824;907 924\"]'\n#     train_df.loc[[1714], 'location'] = '[\"101 129\"]'\n#     train_df.loc[[1929], 'location'] = '[\"531 539;549 561\"]'\n#     train_df.loc[[2134], 'location'] = '[\"540 560;581 593\"]'\n#     train_df.loc[[2191], 'location'] = '[\"32 57\"]'\n#     train_df.loc[[2553], 'location'] = '[\"308 317;376 384\"]'\n#     train_df.loc[[3124], 'location'] = '[\"549 557\"]'\n#     train_df.loc[[3858], 'location'] = '[\"102 123\", \"102 112;125 141\", \"102 112;143 157\", \"102 112;159 171\"]'\n#     train_df.loc[[4373], 'location'] = '[\"33 45\"]'\n#     train_df.loc[[4763], 'location'] = '[\"5 16\"]'\n#     train_df.loc[[4782], 'location'] = '[\"175 194\"]'\n#     train_df.loc[[4908], 'location'] = '[\"700 723\"]'\n#     train_df.loc[[6016], 'location'] = '[\"225 250\"]'\n#     train_df.loc[[6192], 'location'] = '[\"46 69\", \"197 218;236 260\"]'\n#     train_df.loc[[6380], 'location'] = '[\"480 482;507 519\", \"480 482;499 503;512 519\", \"480 482;521 531\", \"480 482;533 545\", \"480 482;564 582\"]'\n#     train_df.loc[[6562], 'location'] = '[\"290 320;327 337\", \"290 320;342 358\"]'\n#     train_df.loc[[6862], 'location'] = '[\"288 296;324 363\"]'\n#     train_df.loc[[7022], 'location'] = '[\"133 182\"]'\n#     train_df.loc[[7422], 'location'] = '[\"102 125\"]'\n#     train_df.loc[[8876], 'location'] = '[\"481 483;533 552\"]'\n#     train_df.loc[[9027], 'location'] = '[\"92 102\", \"123 164\"]'\n#     train_df.loc[[9875], 'location'] = '[\"151 156\", \"163 171\", \"221 225\"]'\n#     train_df.loc[[9938], 'location'] = '[\"89 117\", \"122 138\"]'\n#     train_df.loc[[9973], 'location'] = '[\"344 361\"]'\n#     train_df.loc[[10513], 'location'] = '[\"600 623\"]'\n#     train_df.loc[[11551], 'location'] = '[\"386 400;433 461\"]'\n#     train_df.loc[[11677], 'location'] = '[\"160 201\"]'\n#     train_df.loc[[12124], 'location'] = '[\"325 337;349 366\"]'\n#     train_df.loc[[12279], 'location'] = '[\"405 459;488 524\"]'\n#     train_df.loc[[12289], 'location'] = '[\"353 400;488 524\"]'\n#     train_df.loc[[13238], 'location'] = '[\"293 307\", \"321 331\"]'\n#     train_df.loc[[13297], 'location'] = '[\"182 221\", \"182 213;225 234\"]'\n#     train_df.loc[[13299], 'location'] = '[\"71 96\"]'\n#     train_df.loc[[13845], 'location'] = '[\"86 94;230 256\", \"86 94;237 256\"]'\n#     train_df.loc[[14083], 'location'] = '[\"56 64;156 179\"]'\n\n    return symptoms, pn_notes, train_df","metadata":{"id":"_3skiQgbFTWu","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data loading and torch dataset creation","metadata":{"id":"l1nb1ih3J3cM"}},{"cell_type":"code","source":"# Data directory\nDATA_DIR = '../input/nbme-score-clinical-patient-notes/'\n\nsymptoms = pd.read_csv(DATA_DIR + 'features.csv')\npn_notes = pd.read_csv(DATA_DIR + 'patient_notes.csv')\ntrain_df = pd.read_csv(DATA_DIR + 'train.csv')\n\ntrain_df.drop(['id', 'annotation'], axis=1, inplace=True)\n\nsymptoms['feature_text'] = symptoms['feature_text'].str.replace('-', ' ').str.replace(\" OR \", \"; \")\nsymptoms[\"feature_text\"] = symptoms[\"feature_text\"].apply(lambda x: x.lower())\npn_notes[\"pn_history\"] = pn_notes[\"pn_history\"].apply(lambda x: x.lower())\n\n# Clean up some clear annotation errors\nsymptoms, pn_notes, train_df = clean_datasets(symptoms, pn_notes, train_df)\n\ntrain_df['location'] = train_df['location'].apply(lambda x: ast.literal_eval(x))   # train_df['location'][index] has format \"[\"1 2\"]\"\n\nroberta_tokenizer = RobertaTokenizerFast.from_pretrained('roberta-large')\ndeberta_tokenizer = DebertaV2TokenizerFast.from_pretrained('../input/deberta-tokenizer')\n# Note: offset mapping for these decoders are the same, but DeBERTa tokenizer decodes single tokens without a space, multiple tokens with a space between them\n\ntrain_dataset = symptoms.merge(pn_notes, how='outer', on=['case_num'])\n\n# Model for token-wise classification\nclass TokenModel(nn.Module):\n\n    def __init__(self, cfg, config_path=None):\n        super().__init__()\n        \n        self.config = AutoConfig.from_pretrained(cfg, output_hidden_states=True)\n        if config_path is not None:\n            self.model = AutoModel.from_pretrained(config_path, config=self.config)\n        else:\n            self.model = AutoModel.from_config(self.config)\n\n        if \"roberta\" in cfg:\n            for param in self.model.roberta.parameters():\n                param.requires_grad = False\n        elif \"deberta\" in cfg:\n            for param in self.model.deberta.parameters():\n                param.requires_grad = False\n\n        self.proj_size = 84   # hidden size between 2 linear layers\n        self.fc1 = nn.Linear(self.config.hidden_size, self.proj_size)\n        self.fc2 = nn.Linear(self.proj_size, 2)\n        self.dropout = nn.Dropout(0.1)\n        self.relu = nn.ReLU()\n        \n    def feature(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_states = outputs[0]\n        \n        return last_hidden_states\n\n    def forward(self, inputs):\n        feature = self.feature(inputs)\n        hidden = self.relu(self.fc1(self.dropout(feature)))\n        output = self.fc2(self.dropout(hidden))\n        output.transpose_(1,2)\n        \n        return output\n\n\n# Dataset class for test-set creation\nclass TestDataset(Dataset):\n    \n    def __init__(self, train_dataset, test_samples, tokenizer, public_kaggle_model=False):\n        self.data = train_dataset.merge(test_samples, how='inner', on=['case_num', 'pn_num', 'feature_num'])\n        self.data.reset_index(inplace=True, drop=True)\n        self.tokenizer = tokenizer\n        self.public_kaggle_model = public_kaggle_model\n        \n    def __getitem__(self, index):\n        pn_note = self.data['pn_history'][index]\n\n        if self.public_kaggle_model: length = 354\n        else: length = ModelParams.max_length\n\n        tokenized_text = self.tokenizer(pn_note, self.data['feature_text'][index],\n                                           max_length=length,\n                                           padding='max_length',\n                                           return_offsets_mapping=True,\n                                           return_tensors='pt')\n        \n        # returned outputs from tokenizer have batch size 1, need to reshape\n        for k in tokenized_text.keys():\n            tokenized_text[k] = tokenized_text[k].view(*tokenized_text[k].shape[1:])\n        \n        # Want to zero out the second sentence\n        sep_tok_index = ModelParams.max_length-1\n        for i, v in enumerate(tokenized_text['input_ids']):\n            if v == self.tokenizer.sep_token_id:\n                sep_tok_index = i\n                break\n        tokenized_text['attention_mask'][sep_tok_index+1:] = 0\n        tokenized_text['offset_mapping'] = tokenized_text['offset_mapping'][:sep_tok_index+1]\n        \n        return {'input_ids': tokenized_text['input_ids'], \\\n                'attention_mask': tokenized_text['attention_mask'].type(torch.int8), \\\n                'offset_mapping': tokenized_text['offset_mapping'].type(torch.int16), \\\n                'text': pn_note}\n        \n    def __len__(self):\n        return len(self.data)\n    \nclass ScoringModel(nn.Module):\n    def __init__(self, config_path=None, pretrained=False):\n        super().__init__()\n        \n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(\"microsoft/deberta-v3-large\", output_hidden_states=True)\n        else:\n            self.config = torch.load(config_path)\n        if pretrained:\n            self.model = AutoModel.from_pretrained(\"microsoft/deberta-v3-large\", config=self.config)\n        else:\n            self.model = AutoModel.from_config(self.config)\n        self.fc_dropout = nn.Dropout(0.2)\n        self.fc = nn.Linear(self.config.hidden_size, 1)\n                \n    def feature(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_states = outputs[0]\n        \n        return last_hidden_states\n\n    def forward(self, inputs):\n        feature = self.feature(inputs)\n        output = self.fc(self.fc_dropout(feature))\n        \n        return output\n\n# print([i for i in list(globals().keys()) if len(i) > 0 and i[0] != '_'])\n# del globals()['symptoms'], globals()['pn_notes']","metadata":{"id":"GcYnEvP9UIpp","outputId":"838e7c54-0cbd-4d4d-be28-362c363fdfcd","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Produce predictions for examples not in the original training set for semi-supervised training.","metadata":{"id":"j19DYCDn9QrZ"}},{"cell_type":"code","source":"# All samples we have not classified so far, this is something that needs to be changed for the submission test set\ntest_df = pd.read_csv(DATA_DIR + 'test.csv')\ntest_samples = pd.merge(train_dataset, test_df, on=['case_num', 'pn_num', 'feature_num'], how='inner')\n\ntest_samples.drop(columns=['feature_text', 'pn_history', 'location'], inplace=True)\n\nsoftmax = nn.Softmax(dim=1)\n\nmodels = ['roberta', 'deberta', 'deberta-kaggle']\n\nfor model_type in models:\n    # Change directory names\n    if model_type == 'roberta':\n        test_tokenizer = roberta_tokenizer\n        init_outfolder = '../input/nbme-roberta-large-folds'\n    elif model_type == 'deberta':\n        test_tokenizer = deberta_tokenizer\n        init_outfolder = '../input/nbme-deberta-large-folds'\n    else:\n        test_tokenizer = deberta_tokenizer\n        init_outfolder = '../input/deberta-v3-large-5-folds-public'\n        \n    # The Kaggle Public Model uses a different max length and model architecture so must be treated differently\n    test_dataset = TestDataset(train_dataset, test_samples, cur_tokenizer, public_kaggle_model=(\"kaggle\" in ModelParams.model_type))\n    test_loader = DataLoader(dataset=test_dataset, batch_size=ModelParams.eval_batch_size, num_workers=ModelParams.num_workers, pin_memory=ModelParams.pin_mem)\n    \n    softmax = nn.Softmax(dim=1)\n\n    predictions = []\n    if \"kaggle\" in ModelParams.model_type: folds = list(range(ModelParams.n_folds))\n    else: folds = [1, 2]\n    for fold in folds:\n        if ModelParams.model_type == 'roberta':\n            initial_model = TokenModel('roberta-large')\n        elif ModelParams.model_type == 'deberta':\n            initial_model = TokenModel('microsoft/deberta-v3-large')\n        else:\n            initial_model = ScoringModel(config_path=init_outfolder+'/config.pth', pretrained=False)\n\n        if \"kaggle\" in ModelParams.model_type: initial_model.load_state_dict(torch.load(f\"{init_folder}/deberta-v3-large_fold{fold}_best.pth\")['model'])\n        else: initial_model.load_state_dict(torch.load(f\"{init_folder}/fold_{fold}_weights/model.pth\"))\n\n        for param_group in initial_model.parameters():\n            param_group.data = param_group.data.to(torch.float16)\n            \n        torch.cuda.empty_cache()\n\n        preds = []\n        offset_mappings = []\n        test_texts = []\n        initial_model.eval()\n        initial_model.to(device)\n\n        for inputs in test_loader:\n            offset_mappings.append(inputs['offset_mapping'].numpy())\n            test_texts += inputs['text']\n            del inputs['offset_mapping'], inputs['text']\n            inputs = {k: v.to(device, non_blocking=True) for k, v in inputs.items()}\n            with torch.no_grad():\n                y_preds = initial_model(inputs)\n                if \"kaggle\" in ModelParams.model_type:\n                    temp = y_preds.sigmoid().to('cpu').numpy()\n                else:\n                    temp = softmax(y_preds)[:, 1, :].to('cpu').numpy()\n                preds.append(temp)\n        offset_mappings = np.concatenate(offset_mappings)\n        token_prediction = np.concatenate(preds)\n\n        if \"kaggle\" in ModelParams.model_type: length = 354\n        else: length = ModelParams.max_length\n            \n        token_prediction = token_prediction.reshape((len(test_samples), length))\n        char_probs = get_char_probs(test_texts, token_prediction, offset_mappings)\n        predictions.append(char_probs)\n        del initial_model, token_prediction, char_probs\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    predictions = np.mean(predictions, axis=0)\n    test_samples[\"predictions\"] = predictions\n    test_samples[\"text\"] = test_texts\n    test_samples.to_csv(f\"{model_type}_predictions.csv\", index=False)  ","metadata":{"id":"PqeYKdEK9R8e","outputId":"575ceaab-5cd5-4a3f-a047-db4f84e38a13","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Generate word level labels (locations) given predictions from multiple models","metadata":{"id":"2Ar-RVhU9YN-"}},{"cell_type":"code","source":"best_model_weights = {'roberta': 0.13,\n                      'deberta': 0.27,\n                      'deberta-kaggle': 0.6}\n\nweight_averages = {'roberta': 0.7944,\n                   'deberta': 0.787,\n                   'deberta-kaggle': 0.5}\n\npredictions = []\nfor model_type, w in best_model_weights.items():\n    prediction = pd.read_csv(f\"{model_type}_predictions.csv\")\n    prediction[\"predictions\"] = prediction[\"predictions\"].apply(lambda x: np.array([float(i) for i in x[1:-1].split()]))\n    if 'kaggle' in model_type:\n        max_len_preds = list(prediction[\"predictions\"])\n        for index, row in enumerate(predictions[0]):\n            newlen = len(row) - len(max_len_preds[index])\n            max_len_preds[index] = np.concatenate([max_len_preds[index], np.ones(newlen)*0.335])\n    else:\n        max_len_preds = np.array(prediction[\"predictions\"])\n    predictions.append(max_len_preds*w*0.5/weight_averages[model_type])\n\npredictions = np.sum(predictions, axis=0)\nprediction[\"predictions\"] = predictions\nsemi_supervised_submissions = get_results(prediction)\nsemi_supervised_submissions.to_csv('submission.csv', index=False)","metadata":{"id":"c5AJWPA29abg","trusted":true},"execution_count":null,"outputs":[]}]}